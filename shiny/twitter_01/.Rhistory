runApp()
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
hist(rnom(100, mean=input$mean_1))
hist(rnorm(100, mean=input$mean_1))
runApp('Desktop/Quanta_CS/shiny/twitter_1')
hist(rnorm(100)
)
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
# Define server logic required to draw a histogram
shinyServer(function(input, output) {
output$distplot <- renderPlot({
hist(rnorm(100, mean=input$mean_1))
})
})
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
?rnowm
?rnorm
# generate data
x1 <- rnom(n=1500, mean=input$mean_1, sd=input$std_1)
# generate data
x1 <- rnorm(n=1500, mean=input$mean_1, sd=input$std_1)
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
# Define UI for application that draws a histogram
shinyUI(fluidPage(
titlePanel("Understand Twitter Networks"),
sidebarLayout(
position='right',
sidebarPanel(
#textOutput(inputId='dist_1', "Distribution 1"),
sliderInput(inputId='mean_1',label='Mean 1',min=-6,max=6,value=0,step=0.1),
sliderInput(inputId='std_1',label='Std 1',min=0,max=3,value=1,step=0.1),
sliderInput(inputId='mean_2',label='Mean 2',min=-6,max=6,value=4,step=0.1),
sliderInput(inputId='std_2',label='Std 2',min=0,max=3,value=1,step=0.1),
sliderInput(inputId='bins',label='Number of Bins',min=1,max=100,value=50,step=0.1),
dateRangeInput(
inputId='dates',
label="Search Dates",
start='2010-01-01',
end=Sys.Date(),
min='2006-03-21'),
),
mainPanel(
#textOutput(outputId='main_text', "Finally a Product")
plotOutput("distplot")
)
)
# # Sidebar with a slider input for number of bins
# sidebarLayout(
#     sidebarPanel(
#         sliderInput("bins",
#                     "Number of bins:",
#                     min = 1,
#                     max = 50,
#                     value = 30)
#     ),
#
#     # Show a plot of the generated distribution
#     mainPanel(
#         plotOutput("distPlot")
#     )
# )
))
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_1')
runApp('Desktop/Quanta_CS/shiny/twitter_01')
runApp('Desktop/Quanta_CS/shiny/twitter_01')
library(ggplot2)
library(shiny)
library(rtweet)
## quick overview of rtweet functions
vignette("intro", package = "rtweet")
search_tweets()
lookup_users('jse')
?lookup_users
jse <- lookup_users('jse')
jse
View(jse)
users_data(jse)
users_data(zachcolinwolpe)
users_data('zachcolinwolpe')
users_data(lookup_users('zachcolinwolpe'))
# download their timeline
get_timeline(jse, n=10000)
# download their timeline
get_timeline(jse, n=1000)
# download their timeline
get_timeline(jse, n=100)
# download their timeline
jse <- get_timeline('jse', n=10000)
# download their timeline
jse <- get_timeline('jse', n=10000)
# select the users of interest: influential users SA
user <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 'JP_Verster', 'Nerina_Visser',
'AdrianSaville', 'chrishartZA', 'davidshapiro61')
# select the users of interest: influential users SA
userz <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 'JP_Verster', 'Nerina_Visser',
'AdrianSaville', 'chrishartZA', 'davidshapiro61')
users
# select the users of interest: influential users SA
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 'JP_Verster', 'Nerina_Visser',
'AdrianSaville', 'chrishartZA', 'davidshapiro61')
users
# select the users of interest: influential users SA
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 'JP_Verster', 'Nerina_Visser',
'AdrianSaville', 'chrishartZA', 'davidshapiro61')
users
get_timeline(users, n=1)
# download their timeline
jse <- get_timeline(users, n=1)
View(jse)
# download their timeline
jse_influencers <- get_timeline(users, n=1000)
jse_influencers.head()
head(jse_influencers)
jse_influencers[1,]
names(jse_influencers)
jse['geo_coords'][1]
jse['geo_coords']
jse[['geo_coords']]
# Compile a "Report" on each user
jse_influencers$screen_name
names(jse_influencers)
library(dplyr)
# Compile a "Report" on each user
jse_influencers %>% group_by(screen_name)
?lookup_tweets()
(users[1])
lookup_tweets(users[1])
lookup_users(users[1])
# Compile a "Report" on each user
jse_influencers %>% group_by(screen_name) %>% summarise(n=n())
lookup_users(users[1])
# number of tweets
jse_influencers %>% group_by(screen_name) %>% summarise(n=n())
names(jse_influencers)
jse_influencers[jse_influencers$screen_name == 'WarrenIngram']
jse_influencers[jse_influencers$screen_name == 'WarrenIngram',]
names(jse_influencers[jse_influencers$screen_name == 'WarrenIngram',])
jse_influencers[jse_influencers$screen_name == 'WarrenIngram',]$followers_count
unique(jse_influencers[jse_influencers$screen_name == 'WarrenIngram',]$followers_count)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=followers_count
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=followers_count
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=followers_count
)
unique(jse_influencers[jse_influencers$screen_name == 'WarrenIngram',]$followers_count)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count)
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count)
)
names(jse_influencers)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
retweet_count=retweet_count
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
retweet_count=mean(retweet_count)
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
retweet_count=sum(retweet_count)
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
retweet_count=(retweet_count[1])
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets=mean(retweet_count)
)
View(jse_influencers)
names(jse_influencers)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets=mean(retweet_count),
most_common_location=mode(location)
)
names(jse_influencers)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=favourites_count
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=mean(favourites_count)
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=mean(favourites_count),
dedication_of_followers_prop_likes_per_tweet=
average_likes_per_tweet/n_tweets
)
names(jse_influencers)
jse_influencers$country
names(jse_influencers)
jse_influencers[jse_influencers$screen_name=="warrenIngram"]
jse_influencers[jse_influencers$screen_name=="warrenIngram",]
jse_influencers[jse_influencers$screen_name=="warrenIngram",]$retweet_count
jse_influencers[jse_influencers$screen_name=="warrenIngram",]
jse_influencers[jse_influencers$screen_name=="WarrenIngram",]
jse_influencers[jse_influencers$screen_name=="WarrenIngram",]$retweet_count
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(screen_name) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=mean(favourites_count),
average_retweets=mean(retweet_count)
# dedication_of_followers_prop_likes_per_tweet=
# average_likes_per_tweet/n_tweets
)
# compute net sentiment
jse_influencers %>% group_by(factor(screen_name))
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by(factor(screen_name)) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=mean(favourites_count),
average_retweets=mean(retweet_count)
# dedication_of_followers_prop_likes_per_tweet=
# average_likes_per_tweet/n_tweets
)
# number of tweets, no. followers, no. following (friends),
jse_influencers %>% group_by((screen_name)) %>% summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=mean(favourites_count),
average_retweets=mean(retweet_count)
# dedication_of_followers_prop_likes_per_tweet=
# average_likes_per_tweet/n_tweets
)
# compute net sentiment
jse_influencers %>% group_by(screen_name) $>$ select(text)
# compute net sentiment
jse_influencers %>% group_by(screen_name) %>% select(text)
# compute net sentiment
jse_influencers %>% group_by(screen_name) %>% select(text, screen_name=="WarrenIngram")
# compute net sentiment
jse_influencers %>% group_by(screen_name) %>% select(text) %>% filter(screen_name=="WarrenIngram")
############################################### Clean Text Data ###############################################
text <- jse_influencers %>% group_by(screen_name) %>% filter(screen_name=="WarrenIngram") %>% select(text)
text
############################################### Clean Text Data ###############################################
text <- jse_influencers %>% group_by(screen_name) %>% filter(screen_name=="WarrenIngram") %>% select(text) %>% $text
text$text
############################################### Clean Text Data ###############################################
text <- jse_influencers %>% group_by(screen_name) %>% filter(screen_name=="WarrenIngram") %>% select(text) %>% .$text
text$text
############################################### Clean Text Data ###############################################
text <- jse_influencers %>% group_by(screen_name) %>% filter(screen_name=="WarrenIngram") %>% select(text)
text$text
############################################### Clean Text Data ###############################################
# Resources: https://www.tidytextmining.com/sentiment.html
library(tidytext)
get_sentiments("nrc")
library(textdata)
install.packages(textdata)
install.packages('textdata')
############################################### Clean Text Data ###############################################
# Resources: https://www.tidytextmining.com/sentiment.html
library(tidytext)
get_sentiments("nrc")
runApp('Desktop/Quanta_CS/shiny/twitter_01')
getwd()
setwd("~/Desktop/Quanta_CS/shiny/twitter_01")
nrc_lexicon = get_sentiments("nrc")
head(nrc_lexicon)
nrc_lexicon = get_sentiments("afinn")
Yhead(nrc_lexicon)
head(nrc_lexicon)
nrc_lexicon = get_sentiments("nrc")
afinn_lexicon = get_sentiments("afinn")
getwd()
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv')
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv')
source('~/Desktop/Quanta_CS/shiny/twitter_01/nlp.R', echo=TRUE)
# import lexicon
setwd("~/Desktop/Quanta_CS/shiny/twitter_01")
nrc_lexicon = read.csv('data/lexicon/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicon/afinn_lexicon.csv')
head(afinn_lexicon)
afinn_lexicon
nrc_lexicon
# download lexicon
nrc_lexicon = get_sentiments("nrc")
afinn_lexicon = get_sentiments("afinn")
nrc_lexicon
nrc_lexicon = read.csv('data/lexicon/nrc_lexicon.csv',header=T)
nrc_lexicon
View(afinn_lexicon)
afinn_lexicon = get_sentiments("afinn")
# save lexicon
write.csv2('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicon/afinn_lexicon.csv')
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv')
# download lexicon
nrc_lexicon = get_sentiments("nrc")
afinn_lexicon = get_sentiments("afinn")
?write.csv
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv',row.names=FALSE)
# download lexicon
nrc_lexicon = get_sentiments("nrc")
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv',row.names=FALSE)
nrc_lexicon = read.csv('data/lexicon/nrc_lexicon.csv',header=T)
afinn_lexicon = read.csv('data/lexicon/afinn_lexicon.csv')
source('~/Desktop/Quanta_CS/shiny/twitter_01/nlp.R', echo=TRUE)
read.csv('data/lexicon/nrc_lexicon.csv',header=T)
# download lexicon
nrc_lexicon = get_sentiments("nrc")
nrc_lexicon
data.frame(nrc_lexicon)
source('~/Desktop/Quanta_CS/shiny/twitter_01/nlp.R', echo=TRUE)
read.csv('data/lexicon/nrc_lexicon.csv',header=T)
data.frame(get_sentiments("nrc"))
# download lexicon
nrc_lexicon = data.frame(get_sentiments("nrc"))
nrc_lexicon
head(nrc_lexicon)
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv',col.names = T)
read.csv('data/lexicon/nrc_lexicon.csv',header=T)
# download lexicon
nrc_lexicon = get_sentiments("nrc")
source('~/Desktop/Quanta_CS/shiny/twitter_01/nlp.R', echo=TRUE)
source('~/Desktop/Quanta_CS/shiny/twitter_01/nlp.R', echo=TRUE)
read.csv('data/lexicon/nrc_lexicon.csv',header=T)
read.csv('data/lexicon/nrc_lexicon.csv')
read.csv('data/lexicon/nrc_lexicon.csv')
# download lexicon
nrc_lexicon = get_sentiments("nrc")
afinn_lexicon = get_sentiments("afinn")
class(nrc_lexicon)
(nrc_lexicon)[1,1]
(nrc_lexicon)[1,]
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv', row.names=FALSE, na="")
# download lexicon
nrc_lexicon = get_sentiments("nrc")
# save lexicon
write.csv('nrc_lexicon', 'data/lexicon/nrc_lexicon.csv', row.names=FALSE, na="")
read.csv('data/lexicon/nrc_lexicon.csv')
# save lexicon
write.csv(nrc_lexicon, 'data/lexicon/nrc_lexicon.csv')
read.csv('data/lexicon/nrc_lexicon.csv')
# download lexicon
nrc_lexicon = get_sentiments("nrc")
# save lexicon
write.csv(nrc_lexicon, 'data/lexicon/nrc_lexicon.csv')
read.csv('data/lexicon/nrc_lexicon.csv')
source('~/Desktop/Quanta_CS/shiny/twitter_01/nlp.R', echo=TRUE)
# import lexicon
setwd("~/Desktop/Quanta_CS/shiny/twitter_01")
nrc_lexicon = read.csv('data/lexicon/nrc_lexicon.csv',header=T)
afinn_lexicon = read.csv('data/lexicon/afinn_lexicon.csv')
afinn_lexicon
runApp()
runApp()
# users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 'JP_Verster', 'Nerina_Visser',
'AdrianSaville', 'chrishartZA', 'davidshapiro61')
runApp()
runApp()
runApp('~/Desktop/Quanta_CS/distribution_theory')
runApp()
runApp('~/Desktop/Quanta_CS/distribution_theory')
runApp()
runApp('~/Desktop/Quanta_CS/distribution_theory')
runApp('~/Desktop/Quanta_CS/distribution_theory')
runApp()
runApp()
runApp()
runApp()
library(dplyr)
# get usernames
users <- separate(input$users)
separate(input$users)
library(tidyr)
separate("I dont Know if This is Correct")
separate(data.frame("I dont Know if This is Correct"))
separate(data.frame(c("I dont Know if This is Correct")))
data.frame(c("I dont Know if This is Correct"))
strsplit("I dont Know if This is Correct", " ")
runApp()
runApp()
runApp()
runApp()
strsplit("I dont Know if This is Correct", " ")
runApp()
runApp()
runApp()
runApp()
runApp()
grepl("^\\s*$", 'asdf')
grepl("^\\s*$", 'asdf ')
grepl("^\\s*$", 'asdf     ')
grepl("^s*$", 'asdf     ')
nchar(trimws(' d'))==0
nchar(trimws(' d s'))==0
trim(' afsf a s  a')
/\s/.test("3sdf")
if (/^ *$/.test("wer "))
(/^ *$/.test("wer "))
string.split()
library(stringr)
string.split()
?string.split
scan
scan("23r sf  arf ", what="")
scan(c("23r sf  arf "), what="")
strsplit('dsv sdv sdv ', " ")
strsplit('dsv', " ")
runApp()
strsplit("fd", " ")
runApp()
runApp()
runApp()
runApp()
runApp()
strsplit(character("12 "), " ")
runApp()
strsplit(str(character("12 ")), " ")
strsplit(as.array(character("12 ")), " ")
strsplit((ctoString(character("12 ")), " ")
strsplit((toString(character("12 ")), " ")
strsplit(toString(character("12 ")), " ")
strsplit(toString(character("what is this about then aold by")), " ")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
# contains white space
list(input$users)
runApp()
runApp()
runApp()
users <- strsplit(input$users)
runApp()
runApp()
split(input$users, " ")
runApp()
runApp()
runApp()
runApp()
