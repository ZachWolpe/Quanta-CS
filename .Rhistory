# __________________________________________ compute_bing_sentiment __________________________________________
compute_bing_sentiment <- function(word_list, lexicon, show_plots=FALSE, word_size=30) {
"Compute the Corpus Sentiment for a given Lexicon"
# get bing sentiment
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)
# compute for visualization
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
word_list$score = word_list$pole * word_list$n
# visualize
if (show_plots) {
net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
legend.position = 'none')
print(net_sent)
plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
labels=c("negative",'positive'),
col=c("darkblue", "lightblue"))
print(plt)
# Visualize Most Common Positive vs Negative Words
plt <- word_list %>% group_by(sentiment) %>%
top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
ggplot(aes(reorder(word,n), n, fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y = "Contribution to Sentiment",
x = NULL) +
coord_flip()
print(plt)
# generate wordcloud by sentiment colours
group = c(word_list$sentiment)
basecolours = c('darkgreen','darkred')
colourlist = basecolours[match(group,unique(group))]
plt <- wordcloud(words=word_list$word, freq=word_list$n,
colors=colourlist,
ordered.colors=T,
max.words=word_size)
print(plt)
# Structured sentiment word cloud
plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("darkred",'darkgreen'),
max.words=word_size)
print(plt)
}
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
return(word_list)
}
bing_sent <- compute_bing_sentiment(clean, bing_lexicon, TRUE)
head(bing_sent)
library(shiny); runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
install.packages(‘leaflet’)
install.packages("leaflet")
install.packages("shinyWidgets")
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
install.packages(c("leaflet", "shinyWidgets"))
install.packages(c("leaflet", "shinyWidgets"))
library(shiny); runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
?mainPanel
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
sys.Date()
sys
install.packages("sys")
Sys.Date()
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
Sys.Date()
typeof(Sys.Date())
toString(Sys.Date())
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
toString(Sys.Date())
"2014-01-01"
typeof("2014-01-01")
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
as.character(Sys.Date())
typeof("2014-01-01")
is.character("2014-01-01")
is.character(as.character(Sys.Date()))
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta_CS/shiny/test')
test_test <- function(T) {
print("what the hell?")
}
test_test()
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
library(shiny); runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
get_timeline("ZachColinWolpe", n=3)
get_timeline("ZachColinWolpe", n=3)$text
get_timeline("ZachColinWolpe", n = 100,  parse = TRUE, since_id = "1090887065456148481", max_id = "1092889267674599424", type = "recent") %>%
dplyr::filter(created_at > "2019-01-31" & created_at <="2019-02-04")
tweets <- get_timeline("ZachColinWolpe", n = 100,  parse = TRUE, since_id = "1090887065456148481", max_id = "1092889267674599424", type = "recent") %>%
dplyr::filter(created_at > "2019-01-31" & created_at <="2019-02-04")
tweets <- get_timeline("karinRichards", n = 100,  parse = TRUE, since_id = "1090887065456148481", max_id = "1092889267674599424", type = "recent") %>%
dplyr::filter(created_at > "2019-01-31" & created_at <="2019-02-04")
tweets <- get_timeline("Richards_Karin", n = 100,  parse = TRUE, since_id = "1090887065456148481", max_id = "1092889267674599424", type = "recent") %>%
dplyr::filter(created_at > "2019-01-31" & created_at <="2019-02-04")
tweets
tweets <- get_timeline("Richards_Karin", n = 100,  parse = TRUE, since_id = "1090887065456148481", max_id = "1092889267674599424", type = "recent") %>%
dplyr::filter(created_at > "2019-01-00" & created_at <="2019-12-04")
tweets
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
library(stringr)
trimws(" 23 i ")
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
print(10)
print(10*123)
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
library(shiny); runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
# ______ Summary Statistics ______
summary_stats <- function(tweets) {
user_summaries <- tweets %>% group_by(screen_name) %>%
summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
average_likes_per_tweet=mean(favourites_count),
average_retweets=mean(retweet_count)
)
return(user_summaries)
}
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
table <- summary_stats(get_timeline("ZachColinWolpe"))
table
head(table)
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
iris
typeof(iris)
as.list(table)
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
output$account <-  renderDataTable(
# influencer_stats = summary_stats(tweets=tweets())
#head(influencer_stats)
head(iris)
)
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_influencers <- get_timeline(users, n=2500)
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_influencers <- get_timeline(users, n=2500)
names(jse_influencers)
#  _________________________________________ INFLUENCE REPORT _________________________________________
# number of tweets, no. followers, no. following (friends),
user_summaries <- jse_influencers %>% group_by(screen_name) %>%
summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
# average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=mean(favourites_count),
average_retweets=mean(retweet_count)
# dedication_of_followers_prop_likes_per_tweet=# average_likes_per_tweet/n_tweets
)
user_summaries
library(tidytext)
library(ggplot2)
clean_tweets_text <- function(dataset, plot=FALSE) {
"Return: clean word tokens ranked by frequency of appearance"
# clean & tokenize words
dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)
# remove stop words
dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))
# count word appearences
dataset <- dataset %>% count(word, sort=TRUE)
# customly remove some unwanted terms
dataset <- dataset[dataset$word != 'https',]
# remove numbers
dataset <- dataset[is.na(as.numeric(dataset$word)),]
# order (already done?)
dataset <- dataset %>% mutate(word=reorder(word,n))
if (plot) {
# Most Used Words
plt <- dataset[1:15,] %>%
ggplot(aes(word,n,fill=word)) +
ggtitle("Most Common Words") +
geom_col() +
xlab(NULL) +
coord_flip()
print(plt)
}
return(dataset)
}
# convert dates
#dataset$date = as.Date(dataset$created_at)
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
head(test_data)
clean = clean_tweets_text(test_data, T)
head(clean)
# set location
setwd("/Users/zachwolpe/Desktop/Quanta_CS/MVP/Twitter Influencers")
# set location
getwd()
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
# set location
getwd()
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
# # download lexicon
# nrc_lexicon = get_sentiments("nrc")
# afinn_lexicon = get_sentiments("afinn")
# bing_lexicon = get_sentiments("bing")
#
# # save lexicon
# write.csv(nrc_lexicon, 'data/lexicons/nrc_lexicon.csv')
# write.csv(afinn_lexicon, 'data/lexicons/afinn_lexicon.csv')
# write.csv(bing_lexicon, 'data/lexicons/bing_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('data/lexicons/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicons/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicons/bing_lexicon.csv')
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(wordcloud)
library(reshape2)
# ________________________________________________ Clean Data ________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
# clean text
clean = clean_tweets_text(test_data, F)
# __________________________________________ compute_bing_sentiment __________________________________________
compute_bing_sentiment <- function(word_list, lexicon, show_plots=FALSE, word_size=30) {
"Compute the Corpus Sentiment for a given Lexicon"
# get bing sentiment
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)
# compute for visualization
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
word_list$score = word_list$pole * word_list$n
# visualize
if (show_plots) {
net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
legend.position = 'none')
print(net_sent)
plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
labels=c("negative",'positive'),
col=c("darkblue", "lightblue"))
print(plt)
# Visualize Most Common Positive vs Negative Words
plt <- word_list %>% group_by(sentiment) %>%
top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
ggplot(aes(reorder(word,n), n, fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y = "Contribution to Sentiment",
x = NULL) +
coord_flip()
print(plt)
# generate wordcloud by sentiment colours
group = c(word_list$sentiment)
basecolours = c('darkgreen','darkred')
colourlist = basecolours[match(group,unique(group))]
plt <- wordcloud(words=word_list$word, freq=word_list$n,
colors=colourlist,
ordered.colors=T,
max.words=word_size)
print(plt)
# Structured sentiment word cloud
plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("darkred",'darkgreen'),
max.words=word_size)
print(plt)
}
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
return(word_list)
}
bing_sent <- compute_bing_sentiment(clean, bing_lexicon, TRUE)
head(bing_sent)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(wordcloud)
library(reshape2)
# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
clean = clean_tweets_text(test_data, F)
# ______________________________________________________ Compute Diversified Sentiment ______________________________________________________
diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
"Compute, plot & return the diversified sentiment"
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
# order nrc by 2 criteria: circular position (CLOCK)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
# order
rank <- rank[order(rank$position),]
if (show_plots==T){
# col plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
print(plt)
# coordinate plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() +
ggtitle("Sentiment of Tweets")+
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC'))
print(plt)
}
return(rank)
}
# ______________________________________________________ Call Function ______________________________________________________
diversified_sentiment_analysis(clean, nrc_lexicon)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(wordcloud)
library(reshape2)
# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
clean = clean_tweets_text(test_data, F)
# ______________________________________________________ Compute Diversified Sentiment ______________________________________________________
diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
"Compute, plot & return the diversified sentiment"
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
# order nrc by 2 criteria: circular position (CLOCK)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
# order
rank <- rank[order(rank$position),]
if (show_plots==T){
# col plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
print(plt)
# coordinate plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() +
ggtitle("Sentiment of Tweets")+
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC'))
print(plt)
}
return(rank)
}
# ______________________________________________________ Call Function ______________________________________________________
diversified_sentiment_analysis(clean, nrc_lexicon)
user_summaries
install.packages('rvest')
library("rvest")
install.packages("xml2")
install.packages("xml2")
library(xml2)
library(rvest)
library(xml2)
getwd()
setwd("~/Desktop/Quanta AI")
setwd("~/Desktop/Quanta AI")
setwd("~/Desktop/Quanta AI")
library(rvest)
library(xml2)
setwd("~/Desktop/Quanta AI")
setwd("~/Desktop/Quanta AI/Quanta_CS")
setwd("~/Desktop/Quanta AI/Quanta_CS")
# setwd("~/Desktop/Quanta AI/Quanta_CS")
head(read.csv("data/jse/JSE_top40.csv"))
# setwd("~/Desktop/Quanta AI/Quanta_CS")
jse_top40 <- read.csv("data/jse/JSE_top40.csv")
read.csv("data/jse/JSE_top40.csv")
getwd()
setwd("~/Desktop/Quanta AI/Quanta_CS/data/jse")
# setwd("~/Desktop/Quanta AI/Quanta_CS")
jse_top40 <- read.csv("SE_top40.csv")
setwd("~/Desktop/Quanta AI/Quanta_CS")
read.csv("data/twitter/SA.csv")
getwd()
cd ..
getwd()
getwd()
read.csv("data/jse/JSE_top40.csv")
read.csv("/data/jse/JSE_top40.csv")
read.csv("./data/jse/JSE_top40.csv")
library(readr)
library(readr)
# setwd("~/Desktop/Quanta AI/Quanta_CS")
jse_top40 <- read.csv("./data/jse/JSE_top40.csv")
