print(plt)
}
if (printt) {
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
}
return(word_list)
}
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
# ________________________________________________________ Configure Workspace ________________________________________________________
load_packages <- function() {
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
library(lubridate)
library(ggplot2)
library(stringr)
library(tidyr)
library(wordcloud)
library(reshape2)
}
users
clean = clean_tweets_text(test_data, T)
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_influencers <- get_timeline(users, n=2500)
names(jse_influencers)
#  _________________________________________ INFLUENCE REPORT _________________________________________
# number of tweets, no. followers, no. following (friends),
user_summaries <- jse_influencers %>% group_by(screen_name) %>%
summarise(n_tweets=n(),
followers=unique(followers_count),
following=unique(friends_count),
# average_retweets_per_tweet=mean(retweet_count),
average_likes_per_tweet=mean(favourites_count),
average_retweets=mean(retweet_count)
# dedication_of_followers_prop_likes_per_tweet=# average_likes_per_tweet/n_tweets
)
user_summaries
library(tidytext)
library(ggplot2)
clean_tweets_text <- function(dataset, plot=FALSE) {
"Return: clean word tokens ranked by frequency of appearance"
# clean & tokenize words
dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)
# remove stop words
dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))
# count word appearences
dataset <- dataset %>% count(word, sort=TRUE)
# customly remove some unwanted terms
dataset <- dataset[dataset$word != 'https',]
# remove numbers
dataset <- dataset[is.na(as.numeric(dataset$word)),]
# order (already done?)
dataset <- dataset %>% mutate(word=reorder(word,n))
if (plot) {
# Most Used Words
plt <- dataset[1:15,] %>%
ggplot(aes(word,n,fill=word)) +
ggtitle("Most Common Words") +
geom_col() +
xlab(NULL) +
coord_flip()
print(plt)
}
return(dataset)
}
# convert dates
#dataset$date = as.Date(dataset$created_at)
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
head(test_data)
clean = clean_tweets_text(test_data, T)
head(clean)
# ________________________________________________________ Configure Workspace ________________________________________________________
load_packages <- function() {
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
library(lubridate)
library(ggplot2)
library(stringr)
library(tidyr)
library(wordcloud)
library(reshape2)
}
f <- function(data) {
r = 0
for (i in (1:10000)) {
for (i in (1:length(data))) {
r = r + data[i,1] * data[i,2] / data[i,3] ** data[i,4]
}
}
return(round(r))
}
f(iris)
#create a list with 2 elements
l = (a=1:10, b=11:20)  # the mean of the value in each element
lapply(l, mean)
#create a list with 2 elements
l = (a=(1:10), b=(11:20))  # the mean of the value in each element
#create a list with 2 elements
l = c(a=1:10, b=11:20)  # the mean of the value in each element
l
lapply(l, mean)
class(lapply(l, mean))
lapply(l, mean)
sapply(l, mean)
dim(sapply(l, mean))
dim(lapply(l, mean))
length(lapply(l, mean))
sapply(l, mean)
lapply(l, mean)
library(parallel)
library(parallel)
library(lme4)
detectCores()
system.time(save1 <- lapply(1:100, f))
f <- function(i) {
lmer(Petal.Width ~ . - Species + (1 | Species), data = iris)
}
f <- function(i) {
lmer(Petal.Width ~ . - Species + (1 | Species), data=iris)
}
lmer(Petal.Width ~ . - Species + (1 | Species), data=iris)
f <- function(i) {
lmer(Petal.Width ~ . - Species + (1 | Species), data=iris)
}
system.time(save1 <- lapply(1:100, f))
system.time(save1 <- lapply(1:100, f))
system.time(save2 <- mclapply(1:100, f))
save1
dim(iris)
shiny::runApp('Desktop/twittr-sentiment')
runApp('Desktop/Quanta AI/twittr-sentiment/MVP.R')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
runApp('Desktop/Quanta AI/Shiny App')
realDonaldTrump
TrumpFriends
FriendsTweets
head(FriendsTweets)
dim(FriendsTweets)
unique(FriendsTweets$screen_name)
lengthx(unique(FriendsTweets$screen_name))
length(unique(FriendsTweets$screen_name))
# ________________________________________________________ Configure Workspace ________________________________________________________
load_packages <- function() {
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
library(lubridate)
library(ggplot2)
library(stringr)
library(tidyr)
library(wordcloud)
library(reshape2)
}
load_packages()
library(tidytext)
library(ggplot2)
clean_tweets_text
clean = clean_tweets_text(test_data, T)
()
# set location
getwd()
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
afinn_lexicon = read.csv('data/lexicons/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicons/bing_lexicon.csv')
compute_bing_sentiment <- function(word_list, lexicon=bing_lexicon, show_plots=FALSE, word_size=30, printt=F) {
"Compute the Corpus Sentiment for a given Lexicon"
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)     # get bing sentiment
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)                                      # compute for visualization
word_list$score = word_list$pole * word_list$n
# visualize
if (show_plots) {
net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), legend.position = 'none')
print(net_sent)
plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
labels=c("negative",'positive'), col=c("darkblue", "lightblue"))
print(plt)
# Visualize Most Common Positive vs Negative Words
plt <- word_list %>% group_by(sentiment) %>% top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
ggplot(aes(reorder(word,n), n, fill=sentiment)) + geom_col(show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") + labs(y = "Contribution to Sentiment", x = NULL) + coord_flip()
print(plt)
# generate wordcloud by sentiment colours
group = c(word_list$sentiment)
basecolours = c('darkgreen','darkred')
colourlist = basecolours[match(group,unique(group))]
plt <- wordcloud(words=word_list$word, freq=word_list$n, colors=colourlist,
ordered.colors=T, max.words=word_size)
print(plt)
# Structured sentiment word cloud
plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("darkred",'darkgreen'),
max.words=word_size)
print(plt)
}
if (printt) {
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
}
return(word_list)
}
compute_bing_sentiment <- function(word_list, lexicon=bing_lexicon, show_plots=FALSE, word_size=30, printt=F) {
"Compute the Corpus Sentiment for a given Lexicon"
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)     # get bing sentiment
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)                                      # compute for visualization
word_list$score = word_list$pole * word_list$n
# visualize
if (show_plots) {
net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), legend.position = 'none')
print(net_sent)
plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
labels=c("negative",'positive'), col=c("darkblue", "lightblue"))
print(plt)
# Visualize Most Common Positive vs Negative Words
plt <- word_list %>% group_by(sentiment) %>% top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
ggplot(aes(reorder(word,n), n, fill=sentiment)) + geom_col(show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") + labs(y = "Contribution to Sentiment", x = NULL) + coord_flip()
print(plt)
# generate wordcloud by sentiment colours
group = c(word_list$sentiment)
basecolours = c('darkgreen','darkred')
colourlist = basecolours[match(group,unique(group))]
plt <- wordcloud(words=word_list$word, freq=word_list$n, colors=colourlist,
ordered.colors=T, max.words=word_size)
print(plt)
# Structured sentiment word cloud
plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("darkred",'darkgreen'),
max.words=word_size)
print(plt)
}
if (printt) {
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
}
return(word_list)
}
clean
compute_bing_sentiment(clean, bing_lexicon, TRUE)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(wordcloud)
library(reshape2)
# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
clean = clean_tweets_text(test_data, F)
# ______________________________________________________ Compute Diversified Sentiment ______________________________________________________
diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
"Compute, plot & return the diversified sentiment"
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
# order nrc by 2 criteria: circular position (CLOCK)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
# order
rank <- rank[order(rank$position),]
if (show_plots==T){
# col plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
print(plt)
# coordinate plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() +
ggtitle("Sentiment of Tweets")+
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC'))
print(plt)
}
return(rank)
}
# ______________________________________________________ Call Function ______________________________________________________
diversified_sentiment_analysis(clean, nrc_lexicon)
# set location
getwd()
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
# # download lexicon
# nrc_lexicon = get_sentiments("nrc")
# afinn_lexicon = get_sentiments("afinn")
# bing_lexicon = get_sentiments("bing")
#
# # save lexicon
# write.csv(nrc_lexicon, 'data/lexicons/nrc_lexicon.csv')
# write.csv(afinn_lexicon, 'data/lexicons/afinn_lexicon.csv')
# write.csv(bing_lexicon, 'data/lexicons/bing_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('data/lexicons/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicons/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicons/bing_lexicon.csv')
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(wordcloud)
library(reshape2)
# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
clean = clean_tweets_text(test_data, F)
# ______________________________________________________ Compute Diversified Sentiment ______________________________________________________
diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
"Compute, plot & return the diversified sentiment"
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
# order nrc by 2 criteria: circular position (CLOCK)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
# order
rank <- rank[order(rank$position),]
if (show_plots==T){
# col plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
print(plt)
# coordinate plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() +
ggtitle("Sentiment of Tweets")+
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC'))
print(plt)
}
return(rank)
}
# ______________________________________________________ Call Function ______________________________________________________
diversified_sentiment_analysis(clean, nrc_lexicon)
library(readr)
library(dplyr)
library(ggplot2)
jse_top40 <- read.csv("../../data/jse/jalsh.csv")
jse_top40$Date <- as.Date(jse_top40$Date, "%b %d, %Y")                          # clean Dates
jse_top40$Vol.[grepl("-", jse_top40$Vol.)] = NaN                                # clean Vol.
jse_top40$Vol. <- as.numeric(sub("K", "e3", jse_top40$Vol., fixed=TRUE))
jse_top40$Price <- as.numeric(sub(",", "", jse_top40$Price))                    # clean Price
jse_top40$Change.. <- as.numeric(sub("%", "", jse_top40$Change..))              # clean Change..
jse_top40 <- select(jse_top40, Date, Price, Vol., Change..)                     # select variables
ggplot(jse_top40, aes(x=Date, y=Price)) + geom_line(col="darkblue") + ggtitle("JSE Top 40: Value")
ggplot(jse_top40, aes(x=Date, y=Change..)) + geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")
library(lubridate)
library(rtweet)
library(twitteR)
rt <- c()
since <- as.Date("2019-01-01")
search_tweets('WarrenIngram', n=10, since="2018-01-01", until="2019-01-10")
get_timeline('WarrenIngram',n=10,
since="2018-01-01", until="2018-01-10")
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
print(tweet)
#  score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
print(tweet)
#  score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
FriendsTweets
FriendsTweets %>% group_by(screen_name)
FriendsTweets %>% group_by(screen_name) %>% select(text)
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text()
library(parallel)
mclapply(FriendsTweets %>% group_by(screen_name) %>% select(text), FUN=clean_tweets_text)
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text()
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment()
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n))
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n)) %>% filter(screen_name='BillOReilly')
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n)) %>% filter(screen_name=='BillOReilly')
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n),
sum_scores = sum(score))
TrumpsFriendsSentiment <- FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n), sum_scores = sum(score))
TrumpsFriendsSentiment
wordcloud(TrumpsFriendsSentiment, freq=sum_scores)
TrumpsFriendsSentiment <- FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n), sum_scores = sum(score))
wordcloud(TrumpsFriendsSentiment, freq=sum_scores)
TrumpsFriendsSentiment
wordcloud(TrumpsFriendsSentiment$screen_name, freq=TrumpsFriendsSentiment$sum_scores)
wordcloud(TrumpsFriendsSentiment$screen_name, max.words = 20) # freq=TrumpsFriendsSentiment$sum_scores)
TrumpsFriendsSentiment
wordcloud(words=TrumpsFriendsSentiment$screen_name,
freq=TrumpsFriendsSentiment$sum_scores,
max.words=20)
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n), sum_scores = sum(score)) %>% unique()
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n), sum_scores = sum(score)) %>% unique(screen_name)
TrumpsFriendsSentiment <- FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n), sum_scores = sum(score)) %>% distinct(screen_name)
TrumpsFriendsSentiment
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
mutate(sum_n = sum(n), sum_scores = sum(score))
FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
summarise(sum_scores = sum(score))
TrumpsFriendsSentiment <- FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
clean_tweets_text() %>% compute_bing_sentiment() %>%
summarise(sum_scores = sum(score))
TrumpsFriendsSentiment
wordcloud(words=TrumpsFriendsSentiment$screen_name,
freq=TrumpsFriendsSentiment$sum_scores,
max.words=20)
wordcloud(words=TrumpsFriendsSentiment$screen_name,
freq=TrumpsFriendsSentiment$sum_scores,
max.words=30)
