library(wordcloud)
library(reshape2)
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
# clean text
clean = clean_tweets_text(test_data, F)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(wordcloud)
library(reshape2)
# ________________________________________________ Clean Data ________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
# clean text
clean = clean_tweets_text(test_data, F)
# __________________________________________ compute_bing_sentiment __________________________________________
compute_bing_sentiment <- function(word_list, lexicon=bing_lexicon, show_plots=FALSE, word_size=30, printt=F) {
"Compute the Corpus Sentiment for a given Lexicon"
# get bing sentiment
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)
# compute for visualization
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
word_list$score = word_list$pole * word_list$n
# visualize
if (show_plots) {
net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
legend.position = 'none')
print(net_sent)
plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
labels=c("negative",'positive'),
col=c("darkblue", "lightblue"))
print(plt)
# Visualize Most Common Positive vs Negative Words
plt <- word_list %>% group_by(sentiment) %>%
top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
ggplot(aes(reorder(word,n), n, fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y = "Contribution to Sentiment",
x = NULL) +
coord_flip()
print(plt)
# generate wordcloud by sentiment colours
group = c(word_list$sentiment)
basecolours = c('darkgreen','darkred')
colourlist = basecolours[match(group,unique(group))]
plt <- wordcloud(words=word_list$word, freq=word_list$n,
colors=colourlist,
ordered.colors=T,
max.words=word_size)
print(plt)
# Structured sentiment word cloud
plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("darkred",'darkgreen'),
max.words=word_size)
print(plt)
}
if (printt) {
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
}
return(word_list)
}
bing_sent <- compute_bing_sentiment(clean, bing_lexicon, TRUE)
head(bing_sent)
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(wordcloud)
library(reshape2)
# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
clean = clean_tweets_text(test_data, F)
# ______________________________________________________ Compute Diversified Sentiment ______________________________________________________
diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
"Compute, plot & return the diversified sentiment"
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
# order nrc by 2 criteria: circular position (CLOCK)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
# order
rank <- rank[order(rank$position),]
if (show_plots==T){
# col plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
print(plt)
# coordinate plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() +
ggtitle("Sentiment of Tweets")+
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC'))
print(plt)
}
return(rank)
}
# ______________________________________________________ Call Function ______________________________________________________
diversified_sentiment_analysis(clean, nrc_lexicon)
library(readr)
library(dplyr)
library(ggplot2)
jse_top40 <- read.csv("../../data/jse/JSE_top40.csv")
library(readr)
library(dplyr)
library(ggplot2)
jse_top40 <- read.csv("../../data/jse/JSE_top40.csv")
library(readr)
library(dplyr)
library(ggplot2)
jse_top40 <- read.csv("../../data/jse/JSE_top40.csv")
getwd()
# set location
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
getwd()
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
getwd()
# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
clean = clean_tweets_text(test_data, F)
diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
"Compute, plot & return the diversified sentiment"
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
# order nrc by 2 criteria: circular position (CLOCK)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
# order
rank <- rank[order(rank$position),]
if (show_plots==T){
# col plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
print(plt)
# coordinate plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() +
ggtitle("Sentiment of Tweets")+
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC'))
print(plt)
}
return(rank)
}
# ______________________________________________________ Call Function ______________________________________________________
diversified_sentiment_analysis(clean, nrc_lexicon)
library(ggplot2)
jse_top40 <- read.csv("../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("./data/jse/JSE_top40.csv")
getwd()
jse_top40 <- read.csv("../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("data/jse/JSE_top40.csv")
jse_top40 <- read.csv("./data/jse/JSE_top40.csv")
jse_top40 <- read.csv("../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("../../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("./../../data/jse/JSE_top40.csv")
setwd("~/Desktop/Quanta AI/Quanta_CS")
setwd("~/Desktop/Quanta AI/Quanta_CS")
jse_top40 <- read.csv("./data/jse/JSE_top40.csv")
jse_top40 <- read.csv("../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("/../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("./../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("./data/jse/JSE_top40.csv")
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
getwd()
jse_top40 <- read.csv("../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("/../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("//../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("./../../data/jse/JSE_top40.csv")
jse_top40 <- read.csv("./../../data/jse/jalsh.csv")
jse_top40 <- read.csv("../../data/jse/jalsh.csv")
jse_top40
library(readr)
library(dplyr)
library(ggplot2)
jse_top40 <- read.csv("../../data/jse/jalsh.csv")
jse_top40$Date <- as.Date(jse_top40$Date, "%b %d, %Y")                          # clean Dates
jse_top40$Vol.[grepl("-", jse_top40$Vol.)] = NaN                                # clean Vol.
jse_top40$Vol. <- as.numeric(sub("K", "e3", jse_top40$Vol., fixed=TRUE))
jse_top40$Price <- as.numeric(sub(",", "", jse_top40$Price))                    # clean Price
jse_top40$Change.. <- as.numeric(sub("%", "", jse_top40$Change..))              # clean Change..
jse_top40 <- select(jse_top40, Date, Price, Vol., Change..)                     # select variables
ggplot(jse_top40, aes(x=Date, y=Price)) + geom_line(col="darkblue") + ggtitle("JSE Top 40: Value")
ggplot(jse_top40, aes(x=Date, y=Change..)) + geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")
library(lubridate)
library(rtweet)
library(twitteR)
rt <- c()
since <- as.Date("2019-01-01")
search_tweets('WarrenIngram', n=10, since="2018-01-01", until="2019-01-10")
get_timeline('WarrenIngram',n=10,
since="2018-01-01", until="2018-01-10")
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
print(tweet)
#  score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
# Filter for 2019
library(dplyr)
library(tidytext)
library(lubridate)
library(rtweet)
library(ggplot2)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_tweets_BIG <- get_timeline(users, n=5000)
# ____________________________ COMPUTE SENTIMENT PER WEEK (for 1 year) ____________________________
# create weekly grouping
weekly <- jse_tweets_BIG %>% filter(created_at>="2019-01-01") %>% group_by(screen_name, week=week(created_at)) %>%
select(text, week, created_at, screen_name)
# __________________________________ Not Efficient but Words __________________________________
dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')
sent <- c()
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
sent <- data.frame(sent)
# __________________________________ Compute 1, standardized dataset __________________________________
# ______________ Inner Join ______________
# Remove NaN values
sent[is.na(sent)] <- 0
# Create 1 Dataset
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt[rt$Date>"2015-01-01",], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
from <- "2018-01-01"
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_tweets_BIG <- get_timeline(users, n=5000)
# create weekly grouping
weekly <- jse_tweets_BIG %>% filter(created_at>="2019-01-01") %>% group_by(screen_name, week=week(created_at)) %>%
select(text, week, created_at, screen_name)
dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')
sent <- c()
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
sent <- data.frame(sent)
# Remove NaN values
sent[is.na(sent)] <- 0
# Create 1 Dataset
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt[rt$Date>"2015-01-01",], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
from <- "2018-01-01"
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
cor(rt$net_sentiment, rt$Price)
from <- "2018-01-01"
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', '--') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', 'o-') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue',aes('o-')) +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue',linetype='o-') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue',linetype='dashed') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue',linetype='-o') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue',linetype='dashed') + geom_point() +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', linetype='dashed') + geom_point() +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange', linetype='dashed') + geom_point()
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', linetype='dashed') + geom_point() +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange', linetype='dashed') + geom_point(aes(x=Date, y=stand_net_sentiment), col='orange')
library(parallel)
detectCores()
# nate_silver <- get_timeline('NateSilver538', 10000)
realDonaldTrump <- get_timeline('realDonaldTrump', 10000)
realDonaldTrump
emojis
get_friends(users=realDonaldTrump)
get_friends(users=realDonaldTrump, n=5000, retryonratelimit=T)
TrumpFriends <- get_friends(users=realDonaldTrump, n=5000)
TrumpFriends <- get_friends(users='realDonaldTrump', n=5000)
TrumpFriends
getUser(TrumpFriends$user_id)
as_screenname(TrumpFriends$user_id)
lists_users(TrumpFriends$user_id)
get_timeline(TrumpFriends$user_id, n=1)
getUser(TrumpFriends$user_id)
shiny::runApp('~/Desktop/Shiny App')
install.packages(‘shinydashboard')
install.packages("shinydash")
install.packages("shinydashboard")
library(shiny)
library(shinydashboard)
library(shinythemes)
library(shinyWidgets)
library(shinydashboardPlus)
install.packages("shinydashboardPlus")
library(shiny)
library(shinydashboard)
library(shinythemes)
library(shinyWidgets)
library(shinydashboardPlus)
install.packages("shinythemes")
library(shiny)
library(shinydashboard)
library(shinythemes)
library(shinyWidgets)
library(shinydashboardPlus)
library(shinydashboardPlus)
title <- tags$a(
href="https://www.google.com",
icon("accusoft"),
"QuantaLabs"
)
ui <- dashboardPage(
dashboardHeader(
title = title,
tags$li(class="dropdown", tags$a(href="https://colorlib.com/wp/free-bootstrap-admin-dashboard-templates/",tags$img(src="https://specials-images.forbesimg.com/imageserve/5a836fff31358e4955ad6549/416x416.jpg?background=000000&cropX1=234&cropX2=790&cropY1=7&cropY2=563"), "Peter Thiel", target="_blank")),
tags$li(class="dropdown", tags$a(href="https://colorlib.com/wp/free-bootstrap-admin-dashboard-templates/",icon("envelope"), "Messages", target="_blank")),
tags$li(class="dropdown", tags$a(href="https://colorlib.com/wp/free-bootstrap-admin-dashboard-templates/",icon("bell"), "Notifications", target="_blank"))
),
dashboardSidebar(
chooseSliderSkin(
skin= c("HTML5"),
color="black"
),
textInput("searchBar","Twitter Handle", placeholder = "Please Enter Twiiter Handle"),
sliderInput("slideInput","Number of Tweets", min=10,max=3500, value = 1500),
dateRangeInput("dateInput","Choose Time Period"),
actionButton("submitButton","Search")
),
dashboardBody(
includeCSS("styles.css"),
fluidRow(id="first-row",
tags$div( class="one",
tags$a(href="https://colorlib.com/wp/free-bootstrap-admin-dashboard-templates/",tags$img(src="https://specials-images.forbesimg.com/imageserve/5a836fff31358e4955ad6549/416x416.jpg?background=000000&cropX1=234&cropX2=790&cropY1=7&cropY2=563"),"Peter_Thiel",target="_blank")
),
tags$div( class="two",
tags$p("Hello my name is Phahladira Moja and Im a Computer Science student at the University of Pretria")
)
),
fluidRow(id="second-row",
tags$div(class="sec-row-widget",
widgetUserBox(
title = "Peter Thiel",
subtitle = "Ventue Capitalist",
src = "https://specials-images.forbesimg.com/imageserve/5a836fff31358e4955ad6549/416x416.jpg?background=000000&cropX1=234&cropX2=790&cropY1=7&cropY2=563",
background=TRUE,
# backgroundUrl = "https://images.pexels.com/photos/531880/pexels-photo-531880.jpeg?auto=compress&cs=tinysrgb&h=350",
closable = TRUE,
footer= "Bio: Im a Venture Capitalist who has invested and many start-ups. I am also one of the co-founders of Paypal."
)
),
tags$div(class =" sec-row-value",
valueBox("78%", "Influence", icon = icon("twitter"),color = "green"),
valueBox("32%", "Number of Followers", icon = icon("twitter"),color = "aqua"),
valueBox("46%", "Number of Posts PM", icon = icon("twitter"),color = "teal"),
valueBox("83%", "Average ReTweets PP", icon = icon("twitter"),color = "red"),
valueBox("14%", "Average Engagement PP", icon = icon("twitter"),color = "maroon"),
valueBox("61%", "Average Impact Time", icon = icon("twitter"),color = "purple")
)
),
fluidRow(id="third-row",
tags$div(class="graph",
plotOutput('plot1')
),
tags$div(class="word",
tags$div(id="wordcloud",
plotOutput('plot2')
),
tags$div(id="rose-plot",
plotOutput('plot3')
)
)
)
)
)
server <- function(input, output, session) {
a <- reactive(inpu$slideInput,{
hist(rnorm(input$sliderInput))
})
b <- reactive(inpu$dateInput,{
hist(rnorm(input$dateInput))
})
c <- reactive(inpu$submitButton,{
hist(rnorm(input$searchBar))
})
output$plot1 <- renderPlot({
c()
})
output$plot2 <- renderPlot({
c()
})
output$plot3 <- renderPlot({
c()
})
}
runApp('~/Desktop/Shiny App')
install.packages(‘shinydashboard')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
runApp('~/Desktop/Shiny App')
clean
library(tidytext)
library(ggplot2)
clean_tweets_text <- function(dataset, plot=FALSE) {
"Return: clean word tokens ranked by frequency of appearance"
# clean & tokenize words
dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)
# remove stop words
dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))
# count word appearences
dataset <- dataset %>% count(word, sort=TRUE)
# customly remove some unwanted terms
dataset <- dataset[dataset$word != 'https',]
# remove numbers
dataset <- dataset[is.na(as.numeric(dataset$word)),]
# order (already done?)
dataset <- dataset %>% mutate(word=reorder(word,n))
if (plot) {
# Most Used Words
plt <- dataset[1:15,] %>%
ggplot(aes(word,n,fill=word)) +
ggtitle("Most Common Words") +
geom_col() +
xlab(NULL) +
coord_flip()
print(plt)
}
return(dataset)
}
# convert dates
#dataset$date = as.Date(dataset$created_at)
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin")
head(test_data)
clean = clean_tweets_text(test_data, T)
head(clean)
