---
title: "Twitter Influencers"
output: html_notebook
---



# Do Certain People Yeild Supreme Influence on Twitter?

# Score Influence

Can we compute metrics to _"rank"_ the influencial power of particular members on twitter?

```{r}

library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)



# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 
           'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')



# download their timelines
jse_influencers <- get_timeline(users, n=2500)


names(jse_influencers)

#  _________________________________________ INFLUENCE REPORT _________________________________________

# number of tweets, no. followers, no. following (friends),
user_summaries <- jse_influencers %>% group_by(screen_name) %>% 
  summarise(n_tweets=n(), 
            followers=unique(followers_count),
            following=unique(friends_count),
           # average_retweets_per_tweet=mean(retweet_count),
            average_likes_per_tweet=mean(favourites_count),
            average_retweets=mean(retweet_count)
            # dedication_of_followers_prop_likes_per_tweet=# average_likes_per_tweet/n_tweets
            )

user_summaries



```


# Clean Text
 
"t.co" reference to a link - which is normally an image

```{r}
library(tidytext)
library(ggplot2)
  


clean_tweets_text <- function(dataset, plot=FALSE) {
  "Return: clean word tokens ranked by frequency of appearance"
  
  
  
  # clean & tokenize words
  dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)

  # remove stop words
  dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))

  # count word appearences
  dataset <- dataset %>% count(word, sort=TRUE) 

  # customly remove some unwanted terms
  dataset <- dataset[dataset$word != 'https',]
  
  # remove numbers
  dataset <- dataset[is.na(as.numeric(dataset$word)),]

  # order (already done?)
  dataset <- dataset %>% mutate(word=reorder(word,n))


  if (plot) {
    # Most Used Words
    plt <- dataset[1:15,] %>%
      ggplot(aes(word,n,fill=word)) +
      ggtitle("Most Common Words") + 
      geom_col() +
      xlab(NULL) + 
      coord_flip()
    print(plt)
  }
  
  return(dataset)
  
}


# convert dates
#dataset$date = as.Date(dataset$created_at)
  

test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin") 
head(test_data)

clean = clean_tweets_text(test_data, T)
head(clean)


```


# User Sentiment Analysis

### Download Sentiment Lexicon's
```{r}

# set location
setwd("/Users/zachwolpe/Desktop/Quanta_CS/MVP/Twitter Influencers")

# # download lexicon
# nrc_lexicon = get_sentiments("nrc")
# afinn_lexicon = get_sentiments("afinn")
# bing_lexicon = get_sentiments("bing")
# 
# # save lexicon
# write.csv(nrc_lexicon, 'data/lexicons/nrc_lexicon.csv')
# write.csv(afinn_lexicon, 'data/lexicons/afinn_lexicon.csv')
# write.csv(bing_lexicon, 'data/lexicons/bing_lexicon.csv')

# load lexicon
nrc_lexicon = read.csv('data/lexicons/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicons/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicons/bing_lexicon.csv')

```

### Bing: Polarized Sentiment Analysis

Compute the sentiment of each word in a polarized fashion (_'positive'_ or _'negative'_)

```{r}
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(wordcloud)
library(reshape2)


# ________________________________________________ Clean Data ________________________________________________

test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin") 

# clean text
clean = clean_tweets_text(test_data, F)





# __________________________________________ compute_bing_sentiment __________________________________________

compute_bing_sentiment <- function(word_list, lexicon, show_plots=FALSE, word_size=30) {
  "Compute the Corpus Sentiment for a given Lexicon"
  
  # get bing sentiment
  word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment) 
  
  
  # compute for visualization
  word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
  word_list$score = word_list$pole * word_list$n
  
  
  # visualize
  if (show_plots) {
    net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() + 
      theme(axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      legend.position = 'none')
    print(net_sent)
    
    plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
               labels=c("negative",'positive'),
               col=c("darkblue", "lightblue"))
    print(plt)
    
    # Visualize Most Common Positive vs Negative Words
    plt <- word_list %>% group_by(sentiment) %>%
      top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
      ggplot(aes(reorder(word,n), n, fill=sentiment)) +
      geom_col(show.legend=FALSE) +
      facet_wrap(~sentiment, scales="free_y") +
      labs(y = "Contribution to Sentiment",
           x = NULL) +
      coord_flip()
    print(plt)
    
    
    # generate wordcloud by sentiment colours  
    group = c(word_list$sentiment)
    basecolours = c('darkgreen','darkred')
    colourlist = basecolours[match(group,unique(group))]
    
    plt <- wordcloud(words=word_list$word, freq=word_list$n, 
                     colors=colourlist, 
                     ordered.colors=T,
                     max.words=word_size)
    print(plt)
    
    # Structured sentiment word cloud
    plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>% 
      comparison.cloud(colors=c("darkred",'darkgreen'), 
                       max.words=word_size)
    print(plt)
    
    }
  
  
  print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
  print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
  print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
  
  
  return(word_list)
}


bing_sent <- compute_bing_sentiment(clean, bing_lexicon, TRUE)
head(bing_sent)


        
```


### Afinn: Weighted Sentiment Analysis

Compute the sentiment of each word in a weighted fashion ($\{-5,-4,..,4,5\}$)

```{r}
NULL
```

### nrc: Diversified Sentiment Analysis

Compute the sentiment of each word in a diversified fashion, sentiment words include:

  - trust        
  - fear         
  - negative     
  - sadness     
  - anger        
  - surprise     
  - positive    
  - disgust      
  - joy          
  - anticipation

```{r}
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(wordcloud)
library(reshape2)


# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin") 
clean = clean_tweets_text(test_data, F)


# compute_bing_sentiment <- function(word_list, lexicon, show_plots=FALSE, word_size=30)
  
  
# ______________________________________________________ Compute Diversified Sentiment ______________________________________________________
  
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(clean %>% inner_join(nrc_lexicon) %>% 
                               group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)

# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)


diversed_sentiment



# order nrc
strsplit("anticipation negative sadness trust positive surprise joy fear disgust anger", " ")



```




#### Citations & Resources

```{r}
# _____________________________________ Citations + Resources _____________________________________


# ___________ rTweet ___________
# vignette("intro", package = "rtweet")



# ___________ NLP ___________

# Resources: https://www.tidytextmining.com/index.html

# Lixcon Citation
# article{mohammad13,
#     author = {Mohammad, Saif M. and Turney, Peter D.},
#     title = {Crowdsourcing a Word-Emotion Association Lexicon},
#     journal = {Computational Intelligence},
#     volume = {29},
#     number = {3},
#     pages = {436-465},
#     doi = {10.1111/j.1467-8640.2012.00460.x},
#     url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00460.x},
#     eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00460.x},
#     year = {2013}
# }
```




