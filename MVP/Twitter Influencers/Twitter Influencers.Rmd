---
title: "Twitter Influencers"
output: html_notebook
---



# Do Certain People Yeild Supreme Influence on Twitter?

# Score Influence

Can we compute metrics to _"rank"_ the influencial power of particular members on twitter?

packages
```{r}

load_packages <- function() {
  library(tidytext)
  library(textdata)
  library(rtweet)
  library(dplyr)
  library(lubridate)
  library(ggplot2)
  library(stringr)
  library(tidyr)
  library(wordcloud)
  library(reshape2)
}

load_packages()
```


```{r}

library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)



# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 
           'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')



# download their timelines
jse_influencers <- get_timeline(users, n=2500)


names(jse_influencers)

#  _________________________________________ INFLUENCE REPORT _________________________________________

# number of tweets, no. followers, no. following (friends),
user_summaries <- jse_influencers %>% group_by(screen_name) %>% 
  summarise(n_tweets=n(), 
            followers=unique(followers_count),
            following=unique(friends_count),
           # average_retweets_per_tweet=mean(retweet_count),
            average_likes_per_tweet=mean(favourites_count),
            average_retweets=mean(retweet_count)
            # dedication_of_followers_prop_likes_per_tweet=# average_likes_per_tweet/n_tweets
            )

user_summaries



```


# Clean Text
 
"t.co" reference to a link - which is normally an image

```{r}
library(tidytext)
library(ggplot2)
  


clean_tweets_text <- function(dataset, plot=FALSE) {
  "Return: clean word tokens ranked by frequency of appearance"
  
  
  
  # clean & tokenize words
  dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)

  # remove stop words
  dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))

  # count word appearences
  dataset <- dataset %>% count(word, sort=TRUE) 

  # customly remove some unwanted terms
  dataset <- dataset[dataset$word != 'https',]
  
  # remove numbers
  dataset <- dataset[is.na(as.numeric(dataset$word)),]

  # order (already done?)
  dataset <- dataset %>% mutate(word=reorder(word,n))


  if (plot) {
    # Most Used Words
    plt <- dataset[1:15,] %>%
      ggplot(aes(word,n,fill=word)) +
      ggtitle("Most Common Words") + 
      geom_col() +
      xlab(NULL) + 
      coord_flip()
    print(plt)
  }
  
  return(dataset)
  
}


# convert dates
#dataset$date = as.Date(dataset$created_at)
  

test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin") 
head(test_data)

clean = clean_tweets_text(test_data, T)
head(clean)


```


# User Sentiment Analysis

### Download Sentiment Lexicon's
```{r}

# set location
getwd()
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")

# # download lexicon
# nrc_lexicon = get_sentiments("nrc")
# afinn_lexicon = get_sentiments("afinn")
# bing_lexicon = get_sentiments("bing")
# 
# # save lexicon
# write.csv(nrc_lexicon, 'data/lexicons/nrc_lexicon.csv')
# write.csv(afinn_lexicon, 'data/lexicons/afinn_lexicon.csv')
# write.csv(bing_lexicon, 'data/lexicons/bing_lexicon.csv')

# load lexicon
nrc_lexicon = read.csv('data/lexicons/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicons/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicons/bing_lexicon.csv')

```

### Bing: Polarized Sentiment Analysis

Compute the sentiment of each word in a polarized fashion (_'positive'_ or _'negative'_)

```{r}
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(wordcloud)
library(reshape2)


# ________________________________________________ Clean Data ________________________________________________

test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin") 

# clean text
clean = clean_tweets_text(test_data, F)





# __________________________________________ compute_bing_sentiment __________________________________________

compute_bing_sentiment <- function(word_list, lexicon=bing_lexicon, show_plots=FALSE, word_size=30, printt=F) {
  "Compute the Corpus Sentiment for a given Lexicon"
  
  # get bing sentiment
  word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment) 
  
  
  # compute for visualization
  word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
  word_list$score = word_list$pole * word_list$n
  
  
  # visualize
  if (show_plots) {
    net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() + 
      theme(axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      legend.position = 'none')
    print(net_sent)
    
    plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
               labels=c("negative",'positive'),
               col=c("darkblue", "lightblue"))
    print(plt)
    
    # Visualize Most Common Positive vs Negative Words
    plt <- word_list %>% group_by(sentiment) %>%
      top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
      ggplot(aes(reorder(word,n), n, fill=sentiment)) +
      geom_col(show.legend=FALSE) +
      facet_wrap(~sentiment, scales="free_y") +
      labs(y = "Contribution to Sentiment",
           x = NULL) +
      coord_flip()
    print(plt)
    
    
    # generate wordcloud by sentiment colours  
    group = c(word_list$sentiment)
    basecolours = c('darkgreen','darkred')
    colourlist = basecolours[match(group,unique(group))]
    
    plt <- wordcloud(words=word_list$word, freq=word_list$n, 
                     colors=colourlist, 
                     ordered.colors=T,
                     max.words=word_size)
    print(plt)
    
    # Structured sentiment word cloud
    plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>% 
      comparison.cloud(colors=c("darkred",'darkgreen'), 
                       max.words=word_size)
    print(plt)
    
    }
  
  if (printt) {
  print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
  print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
  print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
  }
  
  return(word_list)
}


bing_sent <- compute_bing_sentiment(clean, bing_lexicon, TRUE)
head(bing_sent)


        
```


### Afinn: Weighted Sentiment Analysis

Compute the sentiment of each word in a weighted fashion ($\{-5,-4,..,4,5\}$)

```{r}
NULL
```

### nrc: Diversified Sentiment Analysis

Compute the sentiment of each word in a diversified fashion, sentiment words include:

  - trust        
  - fear         
  - negative     
  - sadness     
  - anger        
  - surprise     
  - positive    
  - disgust      
  - joy          
  - anticipation

```{r}
library(dplyr)
library(stringr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(wordcloud)
library(reshape2)


# ________________________________________________________ Clean Data ________________________________________________________
test_data <- jse_influencers %>% filter(screen_name=="Richards_Karin") 
clean = clean_tweets_text(test_data, F)



  
# ______________________________________________________ Compute Diversified Sentiment ______________________________________________________

diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
  "Compute, plot & return the diversified sentiment"
  
  
  # compute diversified sentiment - compute the number of appearances of each sentiment type
  diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>% 
                                 group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
  # compute proportion
  diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
  
  # compute ratio
  diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)

  
  
  # order nrc by 2 criteria: circular position (CLOCK)
  rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
                   position=c(0,1,2,3,4,5,6,7,8,9),
                   sent=c(1,1,0,-1,-1,-1,-1,-1,0,1)) 
  rank <- inner_join(diversed_sentiment, rank, by="sentiment")

  # order
  rank <- rank[order(rank$position),]
  
  if (show_plots==T){
      # col plot
      plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
      print(plt)
    
      # coordinate plot
      plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) + 
        geom_bar(stat='identity', show.legend=F) +
        coord_polar() + 
        ggtitle("Sentiment of Tweets")+ 
        xlab("") + ylab("") + 
        theme(legend.position = "None",
              axis.ticks.y = element_blank(),
              axis.line.y = element_blank(),
              axis.text.y = element_blank(),
              plot.title = element_text(hjust = 0.5),
              panel.background = element_rect(fill = "white"),
             panel.grid.major = element_line(colour='#DCDCDC'),
             panel.grid.minor  = element_line(colour='#DCDCDC'))  
      print(plt) 
  }
  return(rank)
}



# ______________________________________________________ Call Function ______________________________________________________
diversified_sentiment_analysis(clean, nrc_lexicon)



```


# Contrast with Markets

```{r}

library(readr)
library(dplyr)
library(ggplot2)


jse_top40 <- read.csv("../../data/jse/jalsh.csv")


jse_top40$Date <- as.Date(jse_top40$Date, "%b %d, %Y")                          # clean Dates
jse_top40$Vol.[grepl("-", jse_top40$Vol.)] = NaN                                # clean Vol. 
jse_top40$Vol. <- as.numeric(sub("K", "e3", jse_top40$Vol., fixed=TRUE))
jse_top40$Price <- as.numeric(sub(",", "", jse_top40$Price))                    # clean Price
jse_top40$Change.. <- as.numeric(sub("%", "", jse_top40$Change..))              # clean Change..       
jse_top40 <- select(jse_top40, Date, Price, Vol., Change..)                     # select variables


ggplot(jse_top40, aes(x=Date, y=Price)) + geom_line(col="darkblue") + ggtitle("JSE Top 40: Value")

ggplot(jse_top40, aes(x=Date, y=Change..)) + geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")

```


# Get Tweets by Date
## Contrast Sentiment over 1 Year

Compute Net Sentiment for Each Week

RETURN TO THIS


```{r}
library(lubridate)
library(rtweet)

library(twitteR)


rt <- c()
since <- as.Date("2019-01-01")

search_tweets('WarrenIngram', n=10, since="2018-01-01", until="2019-01-10")

get_timeline('WarrenIngram',n=10,
             since="2018-01-01", until="2018-01-10")

for (i in (1:5)) {
  
  until <- since
  week(until) <- week(since) + 1
  
  
  # compute 
  tweet <- search_tweets('richards_karin', since=since, until=until)
  print(tweet)
 #  score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
  
  rt <- c(rt, score)
  
  # update dates
  since <- until
  

}


```


# Markets vs Sentiment

```{r}
# Filter for 2019
library(dplyr)
library(tidytext)
library(lubridate)
library(rtweet)
library(ggplot2)

# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin', 
           'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')

# download their timelines
jse_tweets_BIG <- get_timeline(users, n=5000)




# ____________________________ COMPUTE SENTIMENT PER WEEK (for 1 year) ____________________________

# create weekly grouping 
weekly <- jse_tweets_BIG %>% filter(created_at>="2019-01-01") %>% group_by(screen_name, week=week(created_at)) %>% 
  select(text, week, created_at, screen_name)


# __________________________________ Not Efficient but Words __________________________________

dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')


sent <- c()

for (w in (1:(length(dates)-2))) {
  rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
  
  clean <- clean_tweets_text(rt)
  net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
  
  # store results
  sent <- rbind(
    sent,
    data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
  )
}

sent <- data.frame(sent)

# __________________________________ Compute 1, standardized dataset __________________________________



# ______________ Inner Join ______________

# Remove NaN values
sent[is.na(sent)] <- 0


# Create 1 Dataset
rt <- inner_join(
  jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
  sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
  by=c('Date' = "until"))


ggplot(rt[rt$Date>"2015-01-01",], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') + 
  geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')


from <- "2018-01-01"
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', linetype='dashed') + geom_point() + 
  geom_line(aes(x=Date, y=stand_net_sentiment), col='orange', linetype='dashed') + 
  geom_point(aes(x=Date, y=stand_net_sentiment), col='orange') 

cor(rt$net_sentiment, rt$Price)



# _______________________________________________________ Sentiment over Time _______________________________________________________






```


# A person's biggest Inlfluences

 - Who influence as particular person the most? & how much net influence do they have?
 - Get the sentiment of their most influencial accounts (Computational Limits)
 - Score the sentiment of the accounts the user "agrees" with (likes tweets)

Functions:
 
 - get_friends
 - get_favourites


```{R}

# ____________________________________ Compute the Net Influence of ____________________________________

# nate_silver <- get_timeline('NateSilver538', 10000)
realDonaldTrump <- get_timeline('realDonaldTrump', 10000)


TrumpFriends <- get_friends(users='realDonaldTrump')

# Friends Tweets 
FriendsTweets <- get_timeline(TrumpFriends$user_id, n=500)


# _______ _______ _______ _______ compute sentiment _______ _______ _______ _______
# How do Trumps influences predict his sentiment?


library(parallel)



TrumpsFriendsSentiment <- FriendsTweets %>% group_by(screen_name) %>% select(text) %>%
  clean_tweets_text() %>% compute_bing_sentiment() %>%
  summarise(sum_scores = sum(score))

wordcloud(words=TrumpsFriendsSentiment$screen_name, 
          freq=TrumpsFriendsSentiment$sum_scores,
          max.words=30) 




```

#### Citations & Resources

```{r}
# _____________________________________ Citations + Resources _____________________________________


# ___________ rTweet ___________
# vignette("intro", package = "rtweet")



# ___________ NLP ___________

# Resources: https://www.tidytextmining.com/index.html

# Lixcon Citation
# article{mohammad13,
#     author = {Mohammad, Saif M. and Turney, Peter D.},
#     title = {Crowdsourcing a Word-Emotion Association Lexicon},
#     journal = {Computational Intelligence},
#     volume = {29},
#     number = {3},
#     pages = {436-465},
#     doi = {10.1111/j.1467-8640.2012.00460.x},
#     url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8640.2012.00460.x},
#     eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8640.2012.00460.x},
#     year = {2013}
# }
```




