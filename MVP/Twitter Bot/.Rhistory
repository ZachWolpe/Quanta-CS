}
d <- as.Date("2019-01-01")
for (i in (1:52)) {
print(d)
week(d) <- week(d) + 1
print(d)
}
since <- as.Date("2019-01-01")
since
since <- since + week(since) + 1
since <- as.Date("2019-01-01")
since + week(since) + 1
week(since) + 1
since <- as.Date("2019-01-01")
until <- since
week(until) <- week(since) + 1
since
until
rt
library(lubridate)
library(rtweet)
rt <- c()
since <- as.Date("2019-01-01")
for (i in (1:52)) {
until <- since
week(until) <- week(since) + 1
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
rt
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
print(since)
print(until)
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
print(since)
}
rt <- c()
since <- as.Date("2019-01-01")
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
print(since)
print(until)
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
library(lubridate)
library(rtweet)
rt <- c()
since <- as.Date("2019-01-01")
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
print(since)
print(until)
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
compute_bing_sentiment <- function(word_list, lexicon, show_plots=FALSE, word_size=30, printt=F) {
"Compute the Corpus Sentiment for a given Lexicon"
# get bing sentiment
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)
# compute for visualization
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
word_list$score = word_list$pole * word_list$n
# visualize
if (show_plots) {
net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
legend.position = 'none')
print(net_sent)
plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
labels=c("negative",'positive'),
col=c("darkblue", "lightblue"))
print(plt)
# Visualize Most Common Positive vs Negative Words
plt <- word_list %>% group_by(sentiment) %>%
top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
ggplot(aes(reorder(word,n), n, fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y = "Contribution to Sentiment",
x = NULL) +
coord_flip()
print(plt)
# generate wordcloud by sentiment colours
group = c(word_list$sentiment)
basecolours = c('darkgreen','darkred')
colourlist = basecolours[match(group,unique(group))]
plt <- wordcloud(words=word_list$word, freq=word_list$n,
colors=colourlist,
ordered.colors=T,
max.words=word_size)
print(plt)
# Structured sentiment word cloud
plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("darkred",'darkgreen'),
max.words=word_size)
print(plt)
}
if (printt) {
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
}
return(word_list)
}
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
print(since)
print(until)
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
#  score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
library(lubridate)
library(rtweet)
rt <- c()
since <- as.Date("2019-01-01")
for (i in (1:5)) {
until <- since
week(until) <- week(since) + 1
# compute
tweet <- search_tweets('richards_karin', since=since, until=until)
print(tweet)
#  score <- sum(compute_bing_sentiment(clean_tweets_text(richards_karin), bing_lexicon) %>% select(score))
rt <- c(rt, score)
# update dates
since <- until
}
search_tweets('richards_karin', since="2019-01-01", until=("2019-01-20")
search_tweets('richards_karin', since="2019-01-01", until=("2019-01-20"))
library(lubridate)
library(rtweet)
search_tweets('richards_karin', since="2019-01-01", until=("2019-01-20"))
search_tweets('richards_karin', since="2019-01-01", until=("2019-10-20"))
search_tweets('richards_karin', since="2019-01-01", until=("2019-20-20"))
library(rtweet)
search_tweets('richards_karin', since="2019-01-01", until=("2019-20-20"))
search_tweets('richards_karin', since="2019-01-01", until=("2019-12-20"))
search_tweets('richards_karin', since="2019-01-01", until=("2019-12-20"))
search_tweets('richards_karin', since="2019-01-01", until=("2019-01-31"))
search_tweets('WarrenIngram', since="2019-01-01", until=("2019-01-31"))
search_tweets('WarrenIngram', since="2019-01-01", until=("2019-01-31"))
search_tweets('WarrenIngram', since="2019-01-01", until=("2019-01-31"))
search_tweets('WarrenIngram', since="2018-01-01", until=("2019-01-31"))
search_tweets('WarrenIngram', since="2018-01-01", until=("2019-01-31"))
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-31")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-10")
search_tweets('WarrenIngram', since="2018-01-01", until="2019-01-10")
library(twitteR)
searchTwitter('charlie sheen', since='2011-03-01', until='2011-03-02')
search_tweets('WarrenIngram', n=10 since="2018-01-01", until="2019-01-10")
search_tweets('WarrenIngram', n=10, since="2018-01-01", until="2019-01-10")
search_tweets('WarrenIngram', n=10, type="recent", since="2018-01-01", until="2019-01-10")
search_tweets('WarrenIngram', n=100, type="recent", since="2018-01-01", until="2019-01-10")
search_tweets('WarrenIngram', n=100000, type="recent", since="2018-01-01", until="2019-01-10")
sent
# Filter for 2019
library(dplyr)
library(tidytext)
library(lubridate)
library(rtweet)
library(ggplot2)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_tweets_BIG <- get_timeline(users, n=5000)
# ____________________________ COMPUTE SENTIMENT PER WEEK (for 1 year) ____________________________
# create weekly grouping
weekly <- jse_tweets_BIG %>% filter(created_at>="2019-01-01") %>% group_by(screen_name, week=week(created_at)) %>%
select(text, week, created_at, screen_name)
# __________________________________ Not Efficient but Words __________________________________
dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')
sent <- c()
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')
dates
sent <- c()
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
# set location
getwd()
setwd("/Users/zachwolpe/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Influencers")
# # download lexicon
# nrc_lexicon = get_sentiments("nrc")
# afinn_lexicon = get_sentiments("afinn")
# bing_lexicon = get_sentiments("bing")
#
# # save lexicon
# write.csv(nrc_lexicon, 'data/lexicons/nrc_lexicon.csv')
# write.csv(afinn_lexicon, 'data/lexicons/afinn_lexicon.csv')
# write.csv(bing_lexicon, 'data/lexicons/bing_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('data/lexicons/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicons/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicons/bing_lexicon.csv')
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean_tweets_text(rt)
mean(compute_bing_sentiment(clean)[['score']])
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
jse_top40 %>% filter(Date >= dates[1])
sent <- data.frame(sent)
mean(sent$net_sentiment)
ggplot(jse_top40 %>% filter(Date >= "2019-01-01"), aes(x=Date, y=Price)) + geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")
ggplot(jse_top40 %>% filter(Date >= "2019-01-01"), aes(x=Date,
y=(Price-mean(Price))/sd(Price))) + geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")
ggplot(sent, aes(x=sent$until, y=sent$net_sentiment)) + geom_line(col="orange")
ggplot(sent, aes(x=sent$until,
y=(sent$net_sentiment- mean(sent$net_sentiment))/sd(sent$net_sentiment))) +
geom_line(col="orange")
ggplot(sent) +
geom_line(aes(x=sent$until,
y=(sent$net_sentiment- mean(sent$net_sentiment))/sd(sent$net_sentiment)),col="orange")
ggplot(jse_top40 %>% filter(Date >= "2019-01-01"), aes(x=Date,
y=(Price-mean(Price))/sd(Price))) +
geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")+
geom_line(aes(x=sent$until,
y=(sent$net_sentiment- mean(sent$net_sentiment))/sd(sent$net_sentiment)),col="orange")
ggplot(jse_top40 %>% filter(Date >= "2019-01-01"), aes(x=Date,
y=(Price-mean(Price))/sd(Price))) +
geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")
+
geom_line(aes(x=sent$until,
y=(sent$net_sentiment- mean(sent$net_sentiment))/sd(sent$net_sentiment)),col="orange")
geom_line(aes(x=sent$until,
y=(sent$net_sentiment- mean(sent$net_sentiment))/sd(sent$net_sentiment)),col="orange")
# Create 1 Dataset
jse_top40 %>% filter(Date >= "2019-01-01")
# Create 1 Dataset
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price))
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment))
# Create 1 Dataset
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price))
left_join(
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date', "until"))
full_join(
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date', "until"))
full_join(
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
rt <- full_join(
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
rt
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line()
rt
rt <- full_join(
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "since"))
rt
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line() +
geom_line(aes(x=until, y=stand_net_sentiment))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line() +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='lightblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
cor(sent$net_sentiment, rt$Price)
corr(sent$net_sentiment, rt$Price)
rt <- inner_join(
jse_top40 %>% filter(Date >= "2019-01-01") %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
rt
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
corr(rt$net_sentiment, rt$Price)
cor(rt$net_sentiment, rt$Price)
# Inner Join
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
cor(rt$net_sentiment, rt$Price)
# Full Join
rt <- full_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "since"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
# Inner Join
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
# Full Join
rt <- full_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "since"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
# ______________ Inner Join ______________
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
# ______________ Full Join ______________
rt <- full_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "since"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
# ______________ Inner Join ______________
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
# load lexicon
nrc_lexicon = read.csv('./data/lexicons/nrc_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('../data/lexicons/nrc_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('data/lexicons/nrc_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('data/lexicon/nrc_lexicon.csv')
afinn_lexicon = read.csv('./data/lexicon/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicon/bing_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv(./'data/lexicon/nrc_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('./data/lexicon/nrc_lexicon.csv')
bing_lexicon = read.csv('data/lexicon/bing_lexicon.csv')
bing_lexicon = read.csv('../data/lexicon/bing_lexicon.csv')
bing_lexicon = read.csv('../data/lexicon/bing_lexicon.csv')
bing_lexicon = read.csv('/data/lexicon/bing_lexicon.csv')
afinn_lexicon = read.csv('/data/lexicon/afinn_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('/data/lexicon/nrc_lexicon.csv')
getwd()
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Bot")
# ______________________________________________________ Compute Analysis ______________________________________________________
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Bot")
# load lexicon
nrc_lexicon = read.csv('/data/lexicon/nrc_lexicon.csv')
afinn_lexicon = read.csv('/data/lexicon/afinn_lexicon.csv')
# load lexicon
nrc_lexicon = read.csv('./data/lexicon/nrc_lexicon.csv')
afinn_lexicon = read.csv('./data/lexicon/afinn_lexicon.csv')
bing_lexicon = read.csv('./data/lexicon/bing_lexicon.csv')
# ______________________________________________________ Compute Analysis ______________________________________________________
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Bot")
# load lexicon
nrc_lexicon = read.csv('./data/lexicon/nrc_lexicon.csv')
afinn_lexicon = read.csv('./data/lexicon/afinn_lexicon.csv')
bing_lexicon = read.csv('./data/lexicon/bing_lexicon.csv')
clean_tweets_text <- function(dataset) {
"Return: clean word tokens ranked by frequency of appearance"
dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)          # clean & tokenize words
dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))         # remove stop words
dataset <- dataset %>% count(word, sort=TRUE)                              # count word appearences
dataset <- dataset[dataset$word != 'https',]                               # customly remove some unwanted terms
dataset <- dataset[is.na(as.numeric(dataset$word)),]                       # remove numbers
dataset <- dataset %>% mutate(word=reorder(word,n))                        # order (already done?)
return(dataset)
}
compute_bing_sentiment <- function(word_list, lexicon=bing_lexicon) {
"Compute the Corpus Sentiment for a given Lexicon"
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
word_list$score = word_list$pole * word_list$n
return(word_list)
}
# get & clean data
tweets <- get_timeline(n=10000, user=screen_name)
clean_tweets <- clean_tweets_text(tweets)
source('~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Bot/Sentiment Bot.R', echo=TRUE)
rtweet::direct_messages(token=twitter_token)
appname <- "QuantaLabsSentiment"                                         ## name assigned to created app
key <- "szmabseQ9HceMuv0xFQjveaRq"                                       ## api key
secret <- "6gKES04Mrx3nhNdSbmPTtwkRW20yLmmyl1fgfpzFHeWW8pfZ5t"           ## api secret
token <- "1208662769438003201-C5Od4DcVEKC3YKH8x4Tm398yaqpZFi"            ## api access token
token_secret <- "mj81VVg04Tem352M2KzTybXGV3FQRRVGYTsy95rA39geR"          ## api access token secret
key <- "szmabseQ9HceMuv0xFQjveaRq"                                       ## api key
twitter_token <- create_token(
app=appname,
consumer_key=key,
consumer_secret=secret,
access_token=token,
access_secret=token_secret)
rtweet::direct_messages(token=twitter_token)
dm <- direct_messages(token=twitter_token)
dm
names(dm)
dm$events
dm$apps
dm$events$message_create
dm$events$type
dm$events$id
dm$events$message_create
dm$events$message_create$target$recipient_id
dm$events$message_create$target
dm$events$message_create$message_data$text
dm$events$message_create$message_data$entities$hashtags
dm$events$message_create$message_data
dm$events$message_create
dm$events$message_create
dm$events$message_create
names(dm$events$message_create)
names(dm$events$message_create)
str(dm)
dm
dm
dm$events
dm$events[1,]
dm$events[,1]
dm$events[1,]
dm$events[1,][['message_create.sender_id']]
dm$events[1,]$message_create.sender_id
dm$events[1,][message_create.sender_id]
dm$events[1,]['message_create.sender_id']
dm$events[1,]
