# Full Join
rt <- full_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "since"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
# ______________ Inner Join ______________
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
# ______________ Full Join ______________
rt <- full_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "since"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=until, y=stand_net_sentiment), col='orange')
# ______________ Inner Join ______________
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt, aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
FriendsTweets
TrumpFriends
realDonaldTrump
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
library(lubridate)
library(ggplot2)
library(stringr)
library(tidyr)
library(wordcloud)
library(reshape2)
# nate_silver <- get_timeline('NateSilver538', 10000)
realDonaldTrump <- get_timeline('realDonaldTrump', 10000)
# nate_silver <- get_timeline('NateSilver538', 10000)
realDonaldTrump <- get_timeline('realDonaldTrump', 10000)
TrumpFriends <- get_friends(users='realDonaldTrump', n=5000)
TrumpFriends <- get_friends(users='realDonaldTrump')
# Friends Tweets
FriendsTweets <- get_timeline(TrumpFriends$user_id, n=500)
FriendsTweets
unique(FriendsTweets$screen_name)
length(unique(FriendsTweets$screen_name))
head(FriendsTweets)
# Filter for 2019
library(dplyr)
library(tidytext)
library(lubridate)
library(rtweet)
library(ggplot2)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_tweets_BIG <- get_timeline(users, n=5000)
# ____________________________ COMPUTE SENTIMENT PER WEEK (for 1 year) ____________________________
# create weekly grouping
weekly <- jse_tweets_BIG %>% filter(created_at>="2019-01-01") %>% group_by(screen_name, week=week(created_at)) %>%
select(text, week, created_at, screen_name)
# __________________________________ Not Efficient but Words __________________________________
dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')
sent <- c()
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
load_packages()
# ________________________________________________________ Configure Workspace ________________________________________________________
load_packages <- function() {
library(tidytext)
library(textdata)
library(rtweet)
library(dplyr)
library(lubridate)
library(ggplot2)
library(stringr)
library(tidyr)
library(wordcloud)
library(reshape2)
}
load_packages()
library(dplyr)
library(tidytext)
library(lubridate)
library(rtweet)
library(ggplot2)
library(ggplot2)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_tweets_BIG <- get_timeline(users, n=5000)
# create weekly grouping
weekly <- jse_tweets_BIG %>% filter(created_at>="2019-01-01") %>% group_by(screen_name, week=week(created_at)) %>%
select(text, week, created_at, screen_name)
weekly
dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')
dates
sent
sent <- c()
sent
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
library(ggplot2)
clean_tweets_text <- function(dataset, plot=FALSE) {
"Return: clean word tokens ranked by frequency of appearance"
# clean & tokenize words
dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)
# remove stop words
dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))
# count word appearences
dataset <- dataset %>% count(word, sort=TRUE)
# customly remove some unwanted terms
dataset <- dataset[dataset$word != 'https',]
# remove numbers
dataset <- dataset[is.na(as.numeric(dataset$word)),]
# order (already done?)
dataset <- dataset %>% mutate(word=reorder(word,n))
if (plot) {
# Most Used Words
plt <- dataset[1:15,] %>%
ggplot(aes(word,n,fill=word)) +
ggtitle("Most Common Words") +
geom_col() +
xlab(NULL) +
coord_flip()
print(plt)
}
return(dataset)
}
# load lexicon
nrc_lexicon = read.csv('data/lexicons/nrc_lexicon.csv')
afinn_lexicon = read.csv('data/lexicons/afinn_lexicon.csv')
bing_lexicon = read.csv('data/lexicons/bing_lexicon.csv')
compute_bing_sentiment <- function(word_list, lexicon=bing_lexicon, show_plots=FALSE, word_size=30, printt=F) {
"Compute the Corpus Sentiment for a given Lexicon"
# get bing sentiment
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)
# compute for visualization
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
word_list$score = word_list$pole * word_list$n
# visualize
if (show_plots) {
net_sent <- ggplot(word_list, aes(word, score, fill=pole)) + geom_col() +
theme(axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
legend.position = 'none')
print(net_sent)
plt <- pie(x=c(mean(word_list$sentiment=="negative"),mean(word_list$sentiment=="positive")),
labels=c("negative",'positive'),
col=c("darkblue", "lightblue"))
print(plt)
# Visualize Most Common Positive vs Negative Words
plt <- word_list %>% group_by(sentiment) %>%
top_n(n=10, wt=n) %>% mutate(word=reorder(word,n)) %>%
ggplot(aes(reorder(word,n), n, fill=sentiment)) +
geom_col(show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y = "Contribution to Sentiment",
x = NULL) +
coord_flip()
print(plt)
# generate wordcloud by sentiment colours
group = c(word_list$sentiment)
basecolours = c('darkgreen','darkred')
colourlist = basecolours[match(group,unique(group))]
plt <- wordcloud(words=word_list$word, freq=word_list$n,
colors=colourlist,
ordered.colors=T,
max.words=word_size)
print(plt)
# Structured sentiment word cloud
plt <- word_list %>% acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("darkred",'darkgreen'),
max.words=word_size)
print(plt)
}
if (printt) {
print(paste("Proportion of POSITIVE sentiment words: ", mean(word_list$sentiment=="positive")))
print(paste("Proportion of NEGATIVE sentiment words: ", mean(word_list$sentiment=="negative")))
print(paste("Proportion of NEUTRAL sentiment words: ", mean(word_list$sentiment=="neutral")))
}
return(word_list)
}
diversified_sentiment_analysis <- function(word_list, nrc_lexicon, show_plots=TRUE) {
"Compute, plot & return the diversified sentiment"
# compute diversified sentiment - compute the number of appearances of each sentiment type
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
# compute proportion
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
# compute ratio
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
# order nrc by 2 criteria: circular position (CLOCK)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative','anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
# order
rank <- rank[order(rank$position),]
if (show_plots==T){
# col plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=sent)) + geom_col() + ggtitle("Diversified Sentiment")
print(plt)
# coordinate plot
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=ratio, fill=(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() +
ggtitle("Sentiment of Tweets")+
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC'))
print(plt)
}
return(rank)
}
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
sent <- data.frame(sent)
sent
# Remove NaN values
sent[is.na(sent)] <- 0
sent
# Create 1 Dataset
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
jse_top40 <- read.csv("../../data/jse/jalsh.csv")
jse_top40$Date <- as.Date(jse_top40$Date, "%b %d, %Y")                          # clean Dates
jse_top40$Vol.[grepl("-", jse_top40$Vol.)] = NaN                                # clean Vol.
jse_top40$Vol. <- as.numeric(sub("K", "e3", jse_top40$Vol., fixed=TRUE))
jse_top40$Price <- as.numeric(sub(",", "", jse_top40$Price))                    # clean Price
jse_top40$Change.. <- as.numeric(sub("%", "", jse_top40$Change..))              # clean Change..
jse_top40 <- select(jse_top40, Date, Price, Vol., Change..)                     # select variables
ggplot(jse_top40, aes(x=Date, y=Price)) + geom_line(col="darkblue") + ggtitle("JSE Top 40: Value")
ggplot(jse_top40, aes(x=Date, y=Change..)) + geom_line(col="lightblue") + ggtitle("JSE Top 40: Change")
# Create 1 Dataset
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt[rt$Date>"2015-01-01",], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
from <- "2018-01-01"
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', linetype='dashed') + geom_point() +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange', linetype='dashed') +
geom_point(aes(x=Date, y=stand_net_sentiment), col='orange')
# Create 1 Dataset
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt[rt$Date>"2015-01-01",], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
from <- "2018-01-01"
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', linetype='dashed') + geom_point() +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange', linetype='dashed') +
geom_point(aes(x=Date, y=stand_net_sentiment), col='orange')
library(parallel)
# ________________________ ______________________________ Access Tokens & Keys ______________________________________________________
create_my_token <- function() {
appname <- "QuantaLabsSentiment"                                         ## name assigned to created app
key <- "szmabseQ9HceMuv0xFQjveaRq"                                       ## api key
secret <- "6gKES04Mrx3nhNdSbmPTtwkRW20yLmmyl1fgfpzFHeWW8pfZ5t"           ## api secret
token <- "1208662769438003201-C5Od4DcVEKC3YKH8x4Tm398yaqpZFi"            ## api access token
token_secret <- "mj81VVg04Tem352M2KzTybXGV3FQRRVGYTsy95rA39geR"          ## api access token secret
twitter_token <- create_token(
app=appname,
consumer_key=key,
consumer_secret=secret,
access_token=token,
access_secret=token_secret)
return(twitter_token)
}
library(rtweet)
library(dplyr)
library(tidytext)
library(tidyverse)
library(ggplot2)
library(sys)
library(lubridate)
library(randomcoloR)
library(stringr)
library(rtweet)
library(dplyr)
library(tidytext)
library(tidyverse)
library(ggplot2)
library(sys)
library(lubridate)
library(randomcoloR)
library(stringr)
# ________________________ ______________________________ Access Tokens & Keys ______________________________________________________
create_my_token <- function() {
appname <- "QuantaLabsSentiment"                                         ## name assigned to created app
key <- "szmabseQ9HceMuv0xFQjveaRq"                                       ## api key
secret <- "6gKES04Mrx3nhNdSbmPTtwkRW20yLmmyl1fgfpzFHeWW8pfZ5t"           ## api secret
token <- "1208662769438003201-C5Od4DcVEKC3YKH8x4Tm398yaqpZFi"            ## api access token
token_secret <- "mj81VVg04Tem352M2KzTybXGV3FQRRVGYTsy95rA39geR"          ## api access token secret
twitter_token <- create_token(
app=appname,
consumer_key=key,
consumer_secret=secret,
access_token=token,
access_secret=token_secret)
return(twitter_token)
}
# ______________________________________________________ Configure Workspace ______________________________________________________
configure_workspace <- function() {
setwd("~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Bot")
# load lexicon
nrc_lexicon <<- read.csv('./data/lexicon/nrc_lexicon.csv')
afinn_lexicon <<- read.csv('./data/lexicon/afinn_lexicon.csv')
bing_lexicon <<- read.csv('./data/lexicon/bing_lexicon.csv')
}
# ______________________________________________________ Compute Analysis ______________________________________________________
compute_analysis <- function(user_id) {
"Return a users Sentiment Analysis Plot"
clean_tweets_text <- function(dataset) {
"Return: clean word tokens ranked by frequency of appearance"
dataset <- dataset %>% unnest_tokens(word, text) %>% select(word)          # clean & tokenize words
dataset <- dataset %>% anti_join(stop_words, by=c("word", "word"))         # remove stop words
dataset <- dataset %>% count(word, sort=TRUE)                              # count word appearences
dataset <- dataset[dataset$word != 'https',]                               # customly remove some unwanted terms
dataset <- dataset[is.na(as.numeric(dataset$word)),]                       # remove numbers
dataset <- dataset %>% mutate(word=reorder(word,n))                        # order (already done?)
return(dataset)
}
compute_bing_sentiment <- function(word_list, lexicon=bing_lexicon) {
"Compute the Corpus Sentiment for a given Lexicon"
word_list <- word_list %>% inner_join(lexicon, by=c('word','word')) %>% select(word, n, sentiment)
word_list$pole = ifelse(word_list$sentiment == 'positive', 1, -1)
word_list$score = word_list$pole * word_list$n
return(word_list)
}
# get & clean data
tweets <- get_timeline(n=10000, user=user_id)
clean_tweets <- clean_tweets_text(tweets)
# ________ ________ Compute Diversified Sentiment ________ ________
diversified_sentiment_analysis <- function(word_list, nrc_lexicon=nrc_lexicon) {
"Compute, plot & return the diversified sentiment"
diversed_sentiment <- unique(word_list %>% inner_join(nrc_lexicon) %>%
group_by(sentiment) %>% mutate(sum_n = sum(n)) %>% select(sum_n))
diversed_sentiment$proportion = diversed_sentiment$sum_n / sum(diversed_sentiment$sum_n)
diversed_sentiment$ratio = diversed_sentiment$proportion / max(diversed_sentiment$proportion)
rank <- data.frame(sentiment=c('positive','joy','surprise',"sadness",'disgust','negative',
'anger','fear','anticipation','trust'),
position=c(0,1,2,3,4,5,6,7,8,9),
sent=c(1,1,0,-1,-1,-1,-1,-1,0,1))
rank <- inner_join(diversed_sentiment, rank, by="sentiment")
rank <- rank[order(rank$position),]
return(rank)
}
# ____ Call Function ____
rank <- diversified_sentiment_analysis(clean_tweets, nrc_lexicon)
# ________ ________ Sentiment Rose Graph ________ ________
plt <- ggplot(rank, aes(x=reorder(sentiment, position), y=proportion, fill=factor(sent))) +
geom_bar(stat='identity', show.legend=F) +
coord_polar() + ggtitle(sprintf("Sentiment of %s's Tweets!", tweets$screen_name[1])) +
xlab("") + ylab("") +
theme(legend.position = "None",
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(hjust = 0.5),
panel.background = element_rect(fill = "white"),
panel.grid.major = element_line(colour='#DCDCDC'),
panel.grid.minor  = element_line(colour='#DCDCDC')) +
scale_fill_manual(values=c(randomcoloR::randomColor(3)))
# Store Results --?
# return plot
results <- list()
results[['username']] = tweets$screen_name[1]
results[['plot']] = plt
return(results)
}
# _________________________________________________________ Post Tweet _________________________________________________________
post_user_sentiment <- function(twitter_token, screen_name, plt) {
# ___ ___ ___ to make it run ___ ___ ___
is_tweet_length <- function(.x, n = 280) {
.x <- gsub("https?://[[:graph:]]+\\s?", "", .x)
while (grepl("^@\\S+\\s+", .x)) {
.x <- sub("^@\\S+\\s+", "", .x)
}
!(nchar(.x) <= n)   # here's the fix
}
assignInNamespace("is_tweet_length", is_tweet_length, ns = "rtweet")
# ___ ___ ___ to make it run ___ ___ ___
# post tweet
post_tweet(status=paste('Hey @', screen_name, "! Here's your sentiment", sep=""),
media=plt,
token=twitter_token)
}
# ___________________________________________________ Communicate with User: DM ___________________________________________________
compute_on_DMs <- function(twitter_token, compute_analysis) {
# Get DM's
dm <- direct_messages(token=twitter_token)
# for each row (message)
for (i in (1:dim(dm$events)[1])) {
# check if message contains string
if (str_detect(dm$events$message_create[i,]$message_data$text,
regex("please compute my sentiment", ignore_case=TRUE))) {
# set user ID
user_id <- dm$events$message_create[i,]$sender_id
# check if user has NOT already been accounted for (send reply after posted users)
if  (any(!str_detect(dm$events$message_create$message_data$text[dm$events$message_create$target$recipient_id == user_id],
regex("Done! Check my Feed!", ignore_case=TRUE)))
) {
# ____ compute sentiment graph ____
res <- compute_analysis(user_id)
# save plot
sentiment_image_path <- ggsave(filename="temp_sentiment_path.png", plot=res[['plot']])
# ____ create post ____
post_user_sentiment(twitter_token=twitter_token,
screen_name=res$username,
plt='temp_sentiment_path.png')
}
}
}
return(res)
}
# create token
twitter_token <- create_my_token()
# configure workspace
configure_workspace()
# compute results
res <- compute_on_DMs(twitter_token, compute_analysis)
source('~/Desktop/Quanta AI/Quanta_CS/MVP/Twitter Bot/Sentiment Bot.R', echo=TRUE)
# Filter for 2019
library(dplyr)
library(tidytext)
library(lubridate)
library(rtweet)
library(ggplot2)
# initial influencial users
users <- c('WarrenIngram', 'TradersCorner', 'paul_vestact', 'SimonPB', 'Richards_Karin',
'JP_Verster', 'Nerina_Visser','AdrianSaville', 'chrishartZA', 'davidshapiro61')
# download their timelines
jse_tweets_BIG <- get_timeline(users, n=5000)
# ____________________________ COMPUTE SENTIMENT PER WEEK (for 1 year) ____________________________
# create weekly grouping
weekly <- jse_tweets_BIG %>% filter(created_at>="2019-01-01") %>% group_by(screen_name, week=week(created_at)) %>%
select(text, week, created_at, screen_name)
# __________________________________ Not Efficient but Words __________________________________
dates <- seq(from=as.Date("2019-01-01"), to=as.Date("2019-12-31"), by='week')
sent <- c()
for (w in (1:(length(dates)-2))) {
rt <- jse_tweets_BIG %>% filter(created_at>=dates[w], created_at<dates[w+1])
clean <- clean_tweets_text(rt)
net_sentment <- mean(compute_bing_sentiment(clean, bing_lexicon)[['score']])
# store results
sent <- rbind(
sent,
data.frame(net_sentiment=net_sentment, since=dates[w], until=dates[w+1])
)
}
sent <- data.frame(sent)
# __________________________________ Compute 1, standardized dataset __________________________________
# ______________ Inner Join ______________
# Remove NaN values
sent[is.na(sent)] <- 0
# Create 1 Dataset
rt <- inner_join(
jse_top40 %>% filter(Date >= dates[1]) %>% mutate(stand_price=(Price-mean(Price))/sd(Price)),
sent %>% mutate(stand_net_sentiment = (net_sentiment-mean(net_sentiment))/sd(net_sentiment)),
by=c('Date' = "until"))
ggplot(rt[rt$Date>"2015-01-01",], aes(x=Date, y=stand_price)) + geom_line(col='darkblue') +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange')
from <- "2018-01-01"
ggplot(rt[rt$Date>from,], aes(x=Date, y=stand_price)) + geom_line(col='darkblue', linetype='dashed') + geom_point() +
geom_line(aes(x=Date, y=stand_net_sentiment), col='orange', linetype='dashed') +
geom_point(aes(x=Date, y=stand_net_sentiment), col='orange')
cor(rt$net_sentiment, rt$Price)
# _______________________________________________________ Sentiment over Time _______________________________________________________
