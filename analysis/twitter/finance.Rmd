---
title: "The Impact of Key Players in the Twittersphere on financial markets"
output:
  html_notebook: default
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---


##### Twitter Influence Analysis


```{r}
library(dplyr) 
library(SnowballC)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)

library(jsonlite)

# get path to all files in folder
file_names <- list.files(path = "../../data/twitter - finance/", pattern = "json", full.names=TRUE)


# read in individual datasets: dynamic naming 
# for (i in (1:length(file_names))) {
#   assign(x=paste("twitter_",i, sep=""),
#          value=fromJSON(file_names[i]))
# }


# read in  + combine data
data = data.frame()
for (i in file_names) {
  data = rbind(data, fromJSON(i))
}


```

# Clean Data

Process each twitter corpus with the below function.
removal

```{r}




clean_twitter_data <- function(dataset) {
    "Process each Twitter file"
  
    # remove redundant variables
    dataset = select(dataset, c('is_replied','is_reply_to','likes',"parent_tweet_id", 
                                'screen_name',"replies", "retweets","text", 'timestamp'))

    # remove custom patterns
    dataset$text = gsub("â€¦", "", (dataset$text))
    dataset$text = gsub("pdf", "", (dataset$text))
    
    # remove http (url to image)
    dataset$text = gsub('https\\S+\\s*',"", dataset$text)
    
    dataset[['text']] = tolower(dataset[['text']])
    dataset[['text']] = removePunctuation(dataset[['text']])
    dataset[['text']] = stripWhitespace(dataset[['text']])
    dataset[['text']] = removeNumbers(dataset[['text']])
    dataset[['text']] = removeWords(dataset[['text']], words=stopwords("en"))
    dataset[['text']] = stemDocument(dataset[['text']])
  
    
    # convert to DateTime
    dataset["timestamp"] = 
      as.Date(dataset["timestamp"][1:nrow(dataset),1], format="%Y-%m-%d")
    
    return(dataset)
}

# clean 
data = clean_twitter_data(data)


# variables
names(data)

head(data$text)

```


# Distribution of Likes

```{r}
library(ggplot2)

ggplot(data, aes(x=likes)) + geom_histogram(bins=100, col='blue') + 
  ggtitle("Distribution of Likes") + lims(x=c(1,100), y=c(0,600)) 

prop_likes_above <- function(data, thres) {
  "Compute the proportion of likes above a threshold"
  res = c()
  
  for (i in thres) {
    res = c(res,mean(data>i))
    print(paste("proportion of tweets with likes above", i, 'is', round(mean(data>i), digits=4)))
  }
  
  return(data.frame(res))
}


proportions = prop_likes_above(data$likes, seq(100,1000,100))

```


# Understand the Data

Are the number of retweets heavily skewed towards certain users?

 - How many tweets do we see per user?
 - How many average retweets do we see per user?
 - How do these two frequencies relate?
 
```{r}
paste("Number of Unique Accounts Tweeting: ", length(unique(data$screen_name)))

# mean retweets per user & tweets per account
agg_tweets = data %>% select(screen_name, retweets) %>% 
  group_by(screen_name) %>% summarise(mean_retweets = round(mean(retweets)),
                                      appearances = table(screen_name),
                                      n = n()) # same as appearences


head(agg_tweets)


ggplot(agg_tweets, aes(x=screen_name, y=mean_retweets)) + geom_count() + 
  ggtitle("major retweets") +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

ggplot(agg_tweets, aes(x=mean_retweets)) + geom_histogram(bins=100, col='orange') +
  ggtitle("Distribution of Retweets") + lims(x=c(1,50), y=c(0,300)) 


ggplot(agg_tweets, aes(x=screen_name, y=mean_retweets)) + geom_bar(stat="identity") +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + 
  ggtitle("Almost all retweets acrue to a handful of users")


```


It then, may be possible to *weight* users tweets based on their:

  - Average Retweets 
  - Number of Tweets
  - Interaction: Average Retweets x Number of Tweets
  
To more accuractly model the sentiment of the public by expressing sentiment as a weighted function of influencial players in the Twittersphere.

Other possible variable distinctions:

  - own tweet vs retweet influence 
  - replies 
  
  


# Corpus EDA

```{r}
library(plotly)
library(RSentiment)
library(stringr)
library(broom)
library(tidyr)
library(tidytext)
library(NLP)

library(tm)
library(wordcloud)
library(SnowballC)

# proportions
head(prop.table(agg_tweets$mean_retweets),10)
head(prop.table(agg_tweets$n), 10)

library(tinytex)




prepare_for_NLP <- function(text_data) {
  "Prepare Corpus for NLP"
  corpus <- Corpus(VectorSource(text_data))      # create corpus
  myDtm <- DocumentTermMatrix(corpus)           # doc x term matrix
  
  # reduce sparsity
  sparse = removeSparseTerms(myDtm, sparse=.99)
  sparse = as.data.frame(as.matrix(sparse))
  # colnames(sparse) = make.names(colnames(sparse))
  
  # freq of words
  frequency_of_words = colSums(sparse)
  frequency_of_words = frequency_of_words[order(frequency_of_words, decreasing=T)]

  # arrange from most
  #print("Most frequently occuring words:")
  #print(head(frequency_of_words, 7))
  
  # create wordcloud
  wordcloud(freq=as.vector(frequency_of_words), words=names(frequency_of_words),
          colors=brewer.pal(9, "Blues")[4:9])
  
  # store results
  res = list()
  res$myDtm = myDtm
  res$frequency_of_words = frequency_of_words
  rescorpus = corpus
  res$sparse = sparse
  
  return(res)
  
}


res = prepare_for_NLP(data$text)


# visualize most freq words
vis = data.frame(word=names(res$frequency_of_words),
                 freq=as.vector(res$frequency_of_words))

ggplot(vis[1:10,], aes(x=reorder(word, -freq), y=freq, fill=word)) +
  geom_bar(col="black", stat="identity") + xlab("common words") + ylab("n count") + 
  ggtitle("Top 10 Words") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))





# find associated words: great feature of search
print('words associated with "good"')
findAssocs(res$myDtm, c('good'), 0.12)
  


# Cluster the data


# k means clustering
# d = dist(t(as.matrix(myDtm)), method='euclidean')
# kfit = kmeans(d, 3)
# summary(kfit)
# head(kfit)




```

# Graph the Sentiment of each Tweet


Assess the net sentiment of the words in the text by using the built in bing word sentiment dictionary - giving the sentiment of individual words.

Compute the net-sentiment (positive - negative) for the tweets.

### Sentiment of Most Common Words

built in sentiment dictionary:

 - get_sentiments("bing")
 
 
Extremely limited as only words for words with a given sentiment.

```{r}

data$text
# get sentiment of words
sent <- inner_join(get_sentiments("bing"), vis, by="word")
head(sent)


# compute + or - for graph
sent = sent %>% mutate(
  pole = if_else(sentiment=="positive", 1,-1),
  score = pole*freq
  )
head(sent)


sent

ggplot(sent, aes(x=reorder(word,-score), y=score, fill=pole)) + 
  geom_bar(col="black", stat="identity") + ggtitle("Sentiment of Most Common Words") + 
  xlab("common words") + ylab("n count") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))


 

```


# Analysis using Tidy Text: EDA & Visualization

```{r}
library(tidytext)
library(dplyr)
library(stringr)
library(igraph)
library(ggraph)
library(widyr)
library(reshape2)




```



# TF-IDF Tutorial Example

```{r}

# gsub('http.*',"")
# gsub('https.*',"")


library(gutenbergr)
library(tidytext)
library(tidyverse)
library(dplyr)


# download data
physics = gutenberg_download(c(37729, 14725, 13476, 5001), meta_fields='author')

# word count per auther
physics_words = physics %>% unnest_tokens(word, text) %>% 
  count(author, word, sort=TRUE) %>% 
  ungroup()
head(physics_words)

physics_words = physics_words %>% bind_tf_idf(word, author, n)
head(physics_words)

physics_words = physics_words %>% arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels=rev(unique(word)))) %>%
  mutate(author = factor(author, levels=c('Galilei, Galileo',
                                          'Huygens, Christiaan',
                                          'Tesla, Nikola',
                                          'Einstein, Albert')))


# visualize



```



# NLP: Sentiment Analysis

```{r}
library(ggplot2)
library(dplyr)
library(tm)
library(readr)
library(wordcloud)
library(plyr)
library(lubridate)
library(SnowballC)
library(tidyr)
library(tidytext)
library(NLP)



# unclean data 
library(jsonlite)

# get path to all files in folder
file_names <- list.files(path = "../../data/twitter - finance/", pattern = "json", full.names=TRUE)

# read in  + combine data
data = data.frame()
for (i in file_names) {
  data = rbind(data, fromJSON(i))
}

# Clean Data
text <- as.character(data$text)
corpus <- Corpus(VectorSource(text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)
 
freq_up = colSums(as.matrix(dtm))
freq_up = freq_up[order(freq_up, decreasing=TRUE)]
print("Most Common Words")
head(freq_up)


# Sentiment
library(RSentiment)

sentiment_up <- calculate_sentiment(names(freq_up)[1:150])
sentiment_up <- cbind(sentiment_up, as.data.frame(freq_up[1:150]))
names(sentiment_up) = c("text","sentiment","freq")
head(sentiment_up)

# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
  mean(sentiment_up$sentiment == "Negative"),
  mean(sentiment_up$sentiment == "Neutral"),
  mean(sentiment_up$sentiment == "Positive")),
  labels=c("Negative","Neutral","Positive"), main="Proportion of Positive vs Negative words")



# split words
sent_pos = sentiment_up[sentiment_up$sentiment=="Positive",]
sent_neg = sentiment_up[sentiment_up$sentiment=="Negative",]

cat("Negative sentiments: ", sum(sent_neg$freq), " Positive sentiment: ", sum(sent_pos$freq))

# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,
          colors=brewer.pal(9, "Greens")[4:9])
wordcloud(sent_neg$text, sent_neg$freq, random.order=FALSE,
          colors=brewer.pal(9, "Reds")[4:9])




```



# Alternative using Tidyverse

Scraping new data from Twitter. The accounts Musk followers, ranked by how many followers they have.

```{r}
library(ggplot2)
library(dplyr)
library(tm)
library(wordcloud)
library(SnowballC)
library(tidyr)
library(tidytext)
library(NLP)





library(twitteR)
library(ROAuth)
library(httr)
library(base64enc)



# Access Token
token <- list()
token['api_key'] <- "u7LYnq9olFJWNaHxA5OCMHwKE"
token['api_key_secret'] <- "5bEuV3aGh57bBrLQKDxpdKe26DoSCx0EiNiiE78QQQ2mTKpMDx"
token['access_token'] <- '2962567919-BRWjF8qZrNBxH1sAPsReT2epl19HWz8u3cx8Uq0'
token['access_token_secret'] <- '0HHAd04bR7R3y6tjdj7cTmUCeBsEAOwCHSouZsbeli9ad'




# connect 
setup_twitter_oauth(token$api_key, token$api_key_secret, token$access_token, token$access_token_secret)

# get tweets from a user
user = getUser("elonmusk")

# get accounts who user follows
friends = user$getFriends()
head(friends)

friends_df = twListToDF(friends) %>% rownames_to_column()
head(friends_df)


# get top following accounts (based on activity?)
top_fol <- friends_df %>%
  mutate(date=as.Date(created, format="%Y-%m-%d"),
         today=as.Date("2019-12-14", format="%Y-%m-%d"),
         days=as.numeric(today-date),
         statuscount_pday=statusesCount/days) %>%
  select(screenName, followersCount, statuscount_pday) %>%
  arrange(desc(followersCount)) %>%
  .[1:100,]

head(top_fol)


# Clean Data
library(SnowballC)

# top tweets from accounts user is following 
top_fol_tweets <- top_fol %>%
  left_join(select(friends_df, screenName, description), by='screenName') %>% 
  mutate(id = seq_along(1:n()))

tidy_decr <- top_fol_tweets %>%
  unnest_tokens(word, description) %>%
  mutate(word_stem = wordStem(word)) %>%
  anti_join(stop_words, by="word") %>%
  filter(!grepl("\\.|http", word))

head(tidy_decr)

# Tidy wordcloud
library(ggwordcloud)


# Rank by no. followers
tidy_decr = tidy_decr %>% group_by(screenName) %>% select(screenName, followersCount) %>% distinct()
head(tidy_decr)




ggplot(tidy_decr, aes(label=screenName, size=followersCount, color=followersCount)) +
  geom_text_wordcloud() + theme_minimal() + 
  scale_color_gradient(low="darkblue", high='lightblue') + 
  ggtitle('Accounts Musk Follows - sized by their number of followers')
  

```



# Word Frequency in Data 

```{r}

library(twitteR)
library(ROAuth)
library(httr)
library(base64enc)



# Access Token
token <- list()
token['api_key'] <- "u7LYnq9olFJWNaHxA5OCMHwKE"
token['api_key_secret'] <- "5bEuV3aGh57bBrLQKDxpdKe26DoSCx0EiNiiE78QQQ2mTKpMDx"
token['access_token'] <- '2962567919-BRWjF8qZrNBxH1sAPsReT2epl19HWz8u3cx8Uq0'
token['access_token_secret'] <- '0HHAd04bR7R3y6tjdj7cTmUCeBsEAOwCHSouZsbeli9ad'


# download file
download.file(url='https://curl.haxx.se/ca/cacert.pem',
              destfile='cacert.pem')


# connect 
setup_twitter_oauth(token$api_key, token$api_key_secret, token$access_token, token$access_token_secret)


# get tweets 
tweets_may <- searchTwitter('@theresa_may', n=1500)

tweets_df <- do.call('rbind', lapply(tweets_may, as.data.frame))
tweets_df <- sapply(tweets_df, function(x) iconv(x, 'latin1', 'ASCII', sub=""))
tweets_df <- as.data.frame(tweets_df)
head(tweets_df)

library(NLP) 
library(tm)
library(SnowballC)
library(qdap)

may_corpus <- Corpus(VectorSource(tweets_df$text))
freq <- freq_terms(may_corpus)

may_corpus[1]
??freq_terms
plot(freq)

```










