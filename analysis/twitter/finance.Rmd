---
title: "The Impact of Key Players in the Twittersphere on financial markets"
output:
  pdf_document: default
  html_notebook: default
  word_document: default
  html_document:
    df_print: paged
---


##### Twitter Influence Analysis


```{r}
library(dplyr) 
library(SnowballC)
library(NLP)
library(tm)
library(RColorBrewer)
library(wordcloud)

library(jsonlite)

# get path to all files in folder
file_names <- list.files(path = "../../data/twitter - finance/", pattern = "json", full.names=TRUE)


# read in individual datasets: dynamic naming 
# for (i in (1:length(file_names))) {
#   assign(x=paste("twitter_",i, sep=""),
#          value=fromJSON(file_names[i]))
# }


# read in  + combine data
data = data.frame()
for (i in file_names) {
  data = rbind(data, fromJSON(i))
}


```

# Clean Data

Process each twitter corpus with the below function.
removal

```{r}




clean_twitter_data <- function(dataset) {
    "Process each Twitter file"
  
    # remove redundant variables
    dataset = select(dataset, c('is_replied','is_reply_to','likes',"parent_tweet_id", 
                                'screen_name',"replies", "retweets","text", 'timestamp'))

    # remove custom patterns
    dataset$text = gsub("â€¦", "", (dataset$text))
    dataset$text = gsub("pdf", "", (dataset$text))
    
    # remove http (url to image)
    dataset$text = gsub('https\\S+\\s*',"", dataset$text)
    
    dataset[['text']] = tolower(dataset[['text']])
    dataset[['text']] = removePunctuation(dataset[['text']])
    dataset[['text']] = stripWhitespace(dataset[['text']])
    dataset[['text']] = removeNumbers(dataset[['text']])
    dataset[['text']] = removeWords(dataset[['text']], words=stopwords("en"))
    dataset[['text']] = stemDocument(dataset[['text']])
  
    
    # convert to DateTime
    dataset["timestamp"] = 
      as.Date(dataset["timestamp"][1:nrow(dataset),1], format="%Y-%m-%d")
    
    return(dataset)
}

# clean 
data = clean_twitter_data(data)


# variables
names(data)

head(data$text)

```


# Distribution of Likes

```{r}
library(ggplot2)

ggplot(data, aes(x=likes)) + geom_histogram(bins=100, col='blue') + 
  ggtitle("Distribution of Likes") + lims(x=c(1,100), y=c(0,600)) 

prop_likes_above <- function(data, thres) {
  "Compute the proportion of likes above a threshold"
  res = c()
  
  for (i in thres) {
    res = c(res,mean(data>i))
    print(paste("proportion of tweets with likes above", i, 'is', round(mean(data>i), digits=4)))
  }
  
  return(data.frame(res))
}


proportions = prop_likes_above(data$likes, seq(100,1000,100))

```


# Understand the Data

Are the number of retweets heavily skewed towards certain users?

 - How many tweets do we see per user?
 - How many average retweets do we see per user?
 - How do these two frequencies relate?
 
```{r}
paste("Number of Unique Accounts Tweeting: ", length(unique(data$screen_name)))

# mean retweets per user & tweets per account
agg_tweets = data %>% select(screen_name, retweets) %>% 
  group_by(screen_name) %>% summarise(mean_retweets = round(mean(retweets)),
                                      appearances = table(screen_name),
                                      n = n()) # same as appearences


head(agg_tweets)


ggplot(agg_tweets, aes(x=screen_name, y=mean_retweets)) + geom_count() + 
  ggtitle("major retweets") +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

ggplot(agg_tweets, aes(x=mean_retweets)) + geom_histogram(bins=100, col='orange') +
  ggtitle("Distribution of Retweets") + lims(x=c(1,50), y=c(0,300)) 


ggplot(agg_tweets, aes(x=screen_name, y=mean_retweets)) + geom_bar(stat="identity") +
  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank()) + 
  ggtitle("Almost all retweets acrue to a handful of users")


```


It then, may be possible to *weight* users tweets based on their:

  - Average Retweets 
  - Number of Tweets
  - Interaction: Average Retweets x Number of Tweets
  
To more accuractly model the sentiment of the public by expressing sentiment as a weighted function of influencial players in the Twittersphere.

Other possible variable distinctions:

  - own tweet vs retweet influence 
  - replies 
  
  


# Corpus EDA

```{r}
library(plotly)
library(RSentiment)
library(stringr)
library(broom)
library(tidyr)
library(tidytext)
library(NLP)

library(tm)
library(wordcloud)
library(SnowballC)

# proportions
head(prop.table(agg_tweets$mean_retweets),10)
head(prop.table(agg_tweets$n), 10)

library(tinytex)




prepare_for_NLP <- function(text_data) {
  "Prepare Corpus for NLP"
  corpus <- Corpus(VectorSource(text_data))      # create corpus
  myDtm <- DocumentTermMatrix(corpus)           # doc x term matrix
  
  # reduce sparsity
  sparse = removeSparseTerms(myDtm, sparse=.99)
  sparse = as.data.frame(as.matrix(sparse))
  # colnames(sparse) = make.names(colnames(sparse))
  
  # freq of words
  frequency_of_words = colSums(sparse)
  frequency_of_words = frequency_of_words[order(frequency_of_words, decreasing=T)]

  # arrange from most
  print("Most frequently occuring words:")
  print(head(frequency_of_words, 7))
  
  # create wordcloud
  wordcloud(freq=as.vector(frequency_of_words), words=names(frequency_of_words),
          colors=brewer.pal(9, "Blues")[4:9])
  
  # store results
  res = list()
  res$myDtm = myDtm
  res$frequency_of_words = frequency_of_words
  rescorpus = corpus
  res$sparse = sparse
  
  return(res)
  
}


res = prepare_for_NLP(data$text)


# visualize most freq words
vis = data.frame(word=names(res$frequency_of_words),
                 freq=as.vector(res$frequency_of_words))

ggplot(vis[1:10,], aes(x=reorder(word, -freq), y=freq, fill=word)) +
  geom_bar(col="black", stat="identity") + xlab("common words") + ylab("n count") + 
  ggtitle("Top 10 Words") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))





# find associated words: great feature of search
print('words associated with "good"')
findAssocs(res$myDtm, c('good'), 0.12)
  


# Cluster the data
install.packages('fpc')
library(fpc)

# k means clustering
# d = dist(t(as.matrix(myDtm)), method='euclidean')
# kfit = kmeans(d, 3)
# summary(kfit)
# head(kfit)




```

# Graph the Sentiment of each Tweet


Assess the net sentiment of the words in the text by using the built in bing word sentiment dictionary - giving the sentiment of individual words.

Compute the net-sentiment (positive - negative) for the tweets.

### Sentiment of Most Common Words

built in sentiment dictionary:

 - get_sentiments("bing")
 
 
Extremely limited as only words for words with a given sentiment.

```{r}

data$text
# get sentiment of words
sent <- inner_join(get_sentiments("bing"), vis, by="word")
head(sent)


# compute + or - for graph
sent = sent %>% mutate(
  pole = if_else(sentiment=="positive", 1,-1),
  score = pole*freq
  )
head(sent)


sent

ggplot(sent, aes(x=reorder(word,-score), y=score, fill=pole)) + 
  geom_bar(col="black", stat="identity") + ggtitle("Sentiment of Most Common Words") + 
  xlab("common words") + ylab("n count") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))


 

```


# Analysis using Tidy Text: EDA & Visualization

```{r}
library(tidytext)
library(dplyr)
library(stringr)
library(igraph)
library(ggraph)
library(widyr)
library(reshape2)




```



# Climate Change Example

```{r}
library(rtweet)
# gsub('http.*',"")
# gsub('https.*',"")




```








