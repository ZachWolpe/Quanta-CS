# compute + or - for graph
sent = sent %>% mutate(
pole = if_else(sentiment=="positive", 1,-1),
score = pole*freq
)
head(sent)
ggplot(sent, aes(x=reorder(word,score), y=score, fill=pole)) + geom_bar(col="black", stat="identity")
sent
# visualize most freq words
vis = data.frame(word=names(res$frequency_of_words),
freq=as.vector(res$frequency_of_words))
ggplot(vis[1:10,], aes(x=reorder(word, -freq), y=freq, fill=word)) +
geom_bar(col="black", stat="identity") + xlab("common words") + ylab("n count") +
ggtitle("Top 10 Words") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))
# get sentiment of words
sent <- inner_join(get_sentiments("bing"), vis, by="word")
head(sent)
vis
ggplot(sent, aes(x=reorder(word,score), y=score, fill=pole)) +
geom_bar(col="black", stat="identity") + ggtitle("Sentiment of Most Common Words") +
xlab("common words") + ylab("n count") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))
# get sentiment of words
sent <- inner_join(get_sentiments("bing"), vis, by="word")
head(sent)
# compute + or - for graph
sent = sent %>% mutate(
pole = if_else(sentiment=="positive", 1,-1),
score = pole*freq
)
head(sent)
sent
ggplot(sent, aes(x=reorder(word,score), y=score, fill=pole)) +
geom_bar(col="black", stat="identity") + ggtitle("Sentiment of Most Common Words") +
xlab("common words") + ylab("n count") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))
ggplot(sent, aes(x=reorder(word,-score), y=score, fill=pole)) +
geom_bar(col="black", stat="identity") + ggtitle("Sentiment of Most Common Words") +
xlab("common words") + ylab("n count") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))
library(rtweet)
data$text %>% unnest_tokens(word, text)
res$frequency_of_words %>% unnest_tokens(word, text)
vis %>% unnest_tokens(word, text)
install.packages("gutenbergr")
install.packages("gutenbergr")
```
library(gutenbergr)
# gsub('http.*',"")
# gsub('https.*',"")
library(gutenbergr)
library(tidytext)
library(tidyverse)
library(dplyr)
# download data
physics = gutenberg_download(c(37729, 14725, 13476, 5001), meta_fields='author')
# download data
physics = gutenberg_download(c(37729, 14725, 13476, 5001), meta_fields='author')
``
# download data
physics = gutenberg_download(c(37729, 14725, 13476, 5001), meta_fields='author')
# download data
physics = gutenberg_download(c(37729, 14725, 13476, 5001), meta_fields='author')
physics
dim(physics)
physics %>% unnest_tokens(word, text)
physics
physics$text
head(physics)
physics %>% unnest_tokens(word, text)
physics %>% unnest_tokens(word, text) %>%
count(author, word, sort=TRUE)
physics %>% unnest_tokens(word, text) %>%
count(author, word, sort=TRUE) %>%
ungroup()
physics_words = physics %>% unnest_tokens(word, text) %>%
count(author, word, sort=TRUE) %>%
ungroup()
physics_words
head(physics_words)
?bind_tf_idf
physics_words %>% bind_tf_idf(word, author, n)
physics_words = physics_words %>% bind_tf_idf(word, author, n)
head(physics_words)
?desc
?arrange
# visualize
physics_words %>% arrange(desc(tf_idf))
# visualize
physics_words %>% arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels=rev(unique(word))))
physics_words$author %>% unique()
unique(physics_words$author)
'Einstein, Albert)))
# visualize
physics_words %>% arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels=rev(unique(word)))) %>%
mutate(author = factor(author, levels=c('Galilei, Galileo',
physics_words %>% arrange(desc(tf_idf))
physics_words %>% arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels=rev(unique(word))))
physics_words %>% arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels=rev(unique(word)))) %>%
mutate(author = factor(author, levels=c('Galilei, Galileo',
'Huygens, Christiaan',
'Tesla, Nikola',
'Einstein, Albert')))
# visualize
physics_words %>% arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels=rev(unique(word)))) %>%
mutate(author = factor(author, levels=c('Galilei, Galileo',
'Huygens, Christiaan',
'Tesla, Nikola',
'Einstein, Albert')))
library(ggstance)
install.packages(ggstance)
install.packages('ggstance')
install.packages("ggtheme", 'viridis')
install.packages("ggtheme", 'viridis')
ibrary(ggstance)
install.packages("ggtheme")
library(ggtheme)
install.packages('ggstance')
install.packages('ggstance')
library(ggstance)
ggplot(physics_words[1:20,] aes(tf_idf, word, fill=author, alpha=0.5)) +
ggplot(physics_words[1:20,], aes(tf_idf, word, fill=author, alpha=0.5)) +
geom_bar(stat='identity') + labs(title="highest TF-IDF words in classic Physics texts",
x='tf-idf', y=NULL)
# gsub('http.*',"")
# gsub('https.*',"")
library(gutenbergr)
library(tidytext)
library(tidyverse)
library(dplyr)
# download data
physics = gutenberg_download(c(37729, 14725, 13476, 5001), meta_fields='author')
# word count per auther
physics_words = physics %>% unnest_tokens(word, text) %>%
count(author, word, sort=TRUE) %>%
ungroup()
head(physics_words)
physics_words = physics_words %>% bind_tf_idf(word, author, n)
head(physics_words)
physics_words = physics_words %>% arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels=rev(unique(word)))) %>%
mutate(author = factor(author, levels=c('Galilei, Galileo',
'Huygens, Christiaan',
'Tesla, Nikola',
'Einstein, Albert')))
# visualize
library(ggplot2)
library(tw)
library(ggplot2)
library(tm)
library(readr)
library(wordcloud)
library(plyr)
library(lubridate)
library(SnowballC)
library(dplyr, tm)
library(dplyr, tm, readr, ggplot2, wordcloud, plyr, lubridate, SnowballC)
library(dplyr, tm, readr, wordcloud, plyr, lubridate, SnowballC)
library(dplyr)
library(ggplot2)
library(dplyr)
library(tm)
library(readr)
library(wordcloud)
library(plyr)
library(lubridate)
library(SnowballC)
e
# get path to all files in folder
file_names <- list.files(path = "../../data/twitter - finance/", pattern = "json", full.names=TRUE)
# unclean data
library(jsonlite)
# get path to all files in folder
file_names <- list.files(path = "../../data/twitter - finance/", pattern = "json", full.names=TRUE)
# read in  + combine data
data = data.frame()
for (i in file_names) {
data = rbind(data, fromJSON(i))
}
text <- as.character(data$text)
corpus <- Corpus(VectorSource(text))
text <- as.character(data$text)
corpus <- Corpus(VectorSource(text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
DocumentTermMatrix(corpus)
dtm
dtm <- DocumentTermMatrix(corpus)
dtm
colSums(as.matrix(dtm))
freq_up = colSums(as.matrix(dtm))
head(freq_up)
order(freq_up, decreasing=TRUE)
head(order(freq_up, decreasing=TRUE))
head(freq_up[order(freq_up, decreasing=TRUE)])
print("Most Common Words")
head(freq_up[order(freq_up, decreasing=TRUE)])
# Sentiment
calculate_sentiment(names(freq_up))
# Sentiment
library(RSentiment)
calculate_sentiment(names(freq_up))
freq_up = colSums(as.matrix(dtm))
freq_up = colSums(as.matrix(dtm))
freq_up = freq_up[order(freq_up, decreasing=TRUE)]
print("Most Common Words")
head(freq_up)
sentiment_up <- calculate_sentiment(names(freq_up))
cbind(sentiment_up, as.data.frame(freq_up))
calculate_sentiment(names(freq_up))
library(rJava)
install.packages(rJava)
install.packages('rJava')
# Sentiment
library(RSentiment)
library(rJava)
sentiment_up <- calculate_sentiment(names(freq_up))
library(rJava)
install.packages("rJava")
install.packages("rJava")
library(rJava)
library(rJava)
sentiment_up <- calculate_sentiment(names(freq_up))
library(rJava)
sentiment_up <- calculate_sentiment(names(freq_up))
library(rJava)
sentiment_up <- calculate_sentiment(names(freq_up))
library(rJava)
detach("package:rJava", unload = TRUE)
library(rJava)
detach("package:rJava", unload = TRUE)
library(rJava)
# Sentiment
library(RSentiment)
?calculate_sentiment
?RSentiment
??RSentiment
names(freq_up)
calculate_sentiment(corpus)
corpus
data.frame(corpus)
names(freq_up)
calculate_sentiment(names(freq_up))
calculate_sentiment(c("good", "wow", 'crazy', 'scary', 'horrible'))
shape(names(freq_up))
dim(names(freq_up))
length(names(freq_up))
names(freq_up)[1:10]
names(freq_up)[1]
c("good", "wow", 'crazy', 'scary', 'horrible')
sentiment_up <- calculate_sentiment(names(freq_up)[1:100])
sentiment_up <- calculate_sentiment(names(freq_up)[1:1000])
sentiment_up <- calculate_sentiment(names(freq_up)[1:500])
sentiment_up <- calculate_sentiment(names(freq_up)[1:200])
sentiment_up <- calculate_sentiment(names(freq_up)[1:100])
sentiment_up <- calculate_sentiment(names(freq_up)[1:110])
sentiment_up <- calculate_sentiment(names(freq_up)[1:150])
sentiment_up <- cbind(sentiment_up, as.data.frame(freq_up))
sentiment_up <- cbind(sentiment_up, as.data.frame(freq_up[1:150]))
head(sentiment_up)
sentiment_up <- cbind(sentiment_up, freq=as.data.frame(freq_up[1:150]))
sentiment_up <- calculate_sentiment(names(freq_up)[1:150])
sentiment_up <- cbind(sentiment_up, freq=as.data.frame(freq_up[1:150]))
head(sentiment_up)
sentiment_up <- cbind(sentiment_up, as.data.frame(freq=freq_up[1:150]))
sentiment_up <- calculate_sentiment(names(freq_up)[1:150])
sentiment_up <- cbind(sentiment_up, as.data.frame(freq=freq_up[1:150]))
head(sentiment_up)
sentiment_up <- calculate_sentiment(names(freq_up)[1:150])
sentiment_up <- cbind(sentiment_up, as.data.frame(freq=freq_up[1:150]))
sentiment_up <- cbind(sentiment_up, as.data.frame(freq_up[1:150]))
head(sentiment_up)
names(sentiment_up)
names(sentiment_up) = c("text","sentiment","freq")
head(sentiment_up)
# How positive vs Negative is the Data?
sentiment_up$sentiment == "Positive"
# How positive vs Negative is the Data?
mean(sentiment_up$sentiment == "Positive")
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(mean(sentiment_up$sentiment == "Positive"),
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral"))
mean(sentiment_up$sentiment == "Positive")
mean(sentiment_up$sentiment == "Negative")
mean(sentiment_up$sentiment == "Neutral")
mean(sentiment_up$sentiment == "Positive")
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(mean(sentiment_up$sentiment == "Positive"),
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral")))
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
mean(sentiment_up$sentiment == "Neutral"),
mean(sentiment_up$sentiment == "Positive"),
mean(sentiment_up$sentiment == "Negative")),
labels=c("Neutral","Positive","Negative"))
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral"),
mean(sentiment_up$sentiment == "Positive")),
labels=c("Neutral","Positive","Negative"))
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral"),
mean(sentiment_up$sentiment == "Positive")),
labels=c("Negative","Neutral","Positive"))
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral"),
mean(sentiment_up$sentiment == "Positive")),
labels=c("Negative","Neutral","Positive"), main="Proportion of + vs - words")
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral"),
mean(sentiment_up$sentiment == "Positive")),
labels=c("Negative","Neutral","Positive"), main="Proportion of + Positive vs - Negative words")
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral"),
mean(sentiment_up$sentiment == "Positive")),
labels=c("Negative","Neutral","Positive"), main="Proportion of Positive vs Negative words")
# split words
sent_pos = sentiment_up[sentiment_up$sentiment=="Positive",]
sent_neg = sentiment_up[sentiment_up$sentiment=="Negative",]
cat("Negative sentiments: ", sum(sent_neg$freq))
cat("Negative sentiments: ", sum(sent_neg$freq), "Positive sentiment: ", sum(sent_pos$freq))
cat("Negative sentiments: ", sum(sent_neg$freq), " Positive sentiment: ", sum(sent_pos$freq))
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=5, random.order=FALSE)
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE)
library(plotly)
library(RSentiment)
library(stringr)
library(broom)
library(tidyr)
library(tidytext)
library(NLP)
library(tm)
library(wordcloud)
library(SnowballC)
# proportions
head(prop.table(agg_tweets$mean_retweets),10)
head(prop.table(agg_tweets$n), 10)
library(tinytex)
prepare_for_NLP <- function(text_data) {
"Prepare Corpus for NLP"
corpus <- Corpus(VectorSource(text_data))      # create corpus
myDtm <- DocumentTermMatrix(corpus)           # doc x term matrix
# reduce sparsity
sparse = removeSparseTerms(myDtm, sparse=.99)
sparse = as.data.frame(as.matrix(sparse))
# colnames(sparse) = make.names(colnames(sparse))
# freq of words
frequency_of_words = colSums(sparse)
frequency_of_words = frequency_of_words[order(frequency_of_words, decreasing=T)]
# arrange from most
print("Most frequently occuring words:")
print(head(frequency_of_words, 7))
# create wordcloud
wordcloud(freq=as.vector(frequency_of_words), words=names(frequency_of_words),
colors=brewer.pal(9, "Blues")[4:9])
# store results
res = list()
res$myDtm = myDtm
res$frequency_of_words = frequency_of_words
rescorpus = corpus
res$sparse = sparse
return(res)
}
res = prepare_for_NLP(data$text)
# visualize most freq words
vis = data.frame(word=names(res$frequency_of_words),
freq=as.vector(res$frequency_of_words))
ggplot(vis[1:10,], aes(x=reorder(word, -freq), y=freq, fill=word)) +
geom_bar(col="black", stat="identity") + xlab("common words") + ylab("n count") +
ggtitle("Top 10 Words") + guides(fill=F) + theme(plot.title=element_text(hjust=0.5))
# find associated words: great feature of search
print('words associated with "good"')
findAssocs(res$myDtm, c('good'), 0.12)
# Cluster the data
install.packages('fpc')
library(fpc)
# k means clustering
# d = dist(t(as.matrix(myDtm)), method='euclidean')
# kfit = kmeans(d, 3)
# summary(kfit)
# head(kfit)
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,  main='ets',
colors=colors=brewer.pal(9, "Green")[4:9], )
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,  title='ets',
colors=colors=brewer.pal(9, "Green")[4:9], )
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1,   main='ets', random.order=FALSE,
colors=colors=brewer.pal(9, "Green")[4:9], )
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, main='ets', random.order=FALSE,
colors=colors=brewer.pal(9, "Green")[4:9], )
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,
colors=colors=brewer.pal(9, "Green")[4:9], )
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,
colors=brewer.pal(9, "Green")[4:9], )
library(ggplot2)
library(dplyr)
library(tm)
library(readr)
library(wordcloud)
library(plyr)
library(lubridate)
library(SnowballC)
library(tidyr)
library(tidytext)
library(NLP)
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,
colors=brewer.pal(9, "Green")[4:9], )
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE, main="Positive Words")
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE, main="Positive Words",
colors=brewer.pal(9, "Greens")[4:9] )
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE, main="Positive Words",
colors=brewer.pal(9, "Greens")[4:9] )
text(main="Positive Words")
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,
colors=brewer.pal(9, "Greens")[4:9])
text(main="Positive Words")
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,
colors=brewer.pal(9, "Greens")[4:9])
wordcloud(sent_neg$text, sent_neg$freq, min.freq=1, random.order=FALSE,
colors=brewer.pal(9, "Reds")[4:9])
wordcloud(sent_neg$text, sent_neg$freq, random.order=FALSE,
colors=brewer.pal(9, "Reds")[4:9])
# Create Postive & Negative Word Cloud
wordcloud(sent_pos$text, sent_pos$freq, min.freq=1, random.order=FALSE,
colors=brewer.pal(9, "Greens")[4:9])
# How positive vs Negative is the Data?
#print(paste("Proportion of ",mean(sentiment_up$sentiment == "Positive"))
pie(c(
mean(sentiment_up$sentiment == "Negative"),
mean(sentiment_up$sentiment == "Neutral"),
mean(sentiment_up$sentiment == "Positive")),
labels=c("Negative","Neutral","Positive"), main="Proportion of Positive vs Negative words")
library(ggplot2)
library(dplyr)
library(tm)
library(readr)
library(wordcloud)
library(plyr)
library(lubridate)
library(SnowballC)
library(tidyr)
library(tidytext)
library(NLP)
# unclean data
library(jsonlite)
# get path to all files in folder
file_names <- list.files(path = "../../data/twitter - finance/", pattern = "json", full.names=TRUE)
# read in  + combine data
data = data.frame()
for (i in file_names) {
data = rbind(data, fromJSON(i))
}
# Clean Data
text <- as.character(data$text)
corpus <- Corpus(VectorSource(text))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)
freq_up = colSums(as.matrix(dtm))
freq_up = freq_up[order(freq_up, decreasing=TRUE)]
print("Most Common Words")
head(freq_up)
# Sentiment
library(RSentiment)
sentiment_up <- calculate_sentiment(names(freq_up)[1:150])
sentiment_up <- cbind(sentiment_up, as.data.frame(freq_up[1:150]))
names(sentiment_up) = c("text","sentiment","freq")
head(sentiment_up)
getwd()
setwd("~/Desktop/Quanta_CS/analysis/twitter")
setwd("~/Desktop/Quanta_CS/analysis/twitter")
getwd()
